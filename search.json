[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "",
    "text": "NoteAI usage disclosure\n\n\n\n\n\nWe used Claude (Sonnet 4.5) to help write, clean up, and comment our code; we also used it to review our code and selectively incorporated its feedback. Additionally, we used Gemini (3.0 Pro) to suggest wording ideas and then chose which small phrases or sentence structure ideas to use, write text that we edited, rearrange starting text to fit the structure of one of our pub templates, expand on summary text that we provided and then edited the text it produced, and help copy-edit draft text to match Arcadia’s style."
  },
  {
    "objectID": "index.html#purpose",
    "href": "index.html#purpose",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Purpose",
    "text": "Purpose\nAuditing the evolutionary information encoded by MSA Pairformer is a critical step in understanding the behavior of protein language models (pLMs) that leverage external context in the form of MSAs. In this study, we bridge MSA Pairformer with classical phylogenetics to quantify how the model’s internal sequence weighting maps onto inferred phylogenetic trees. We show that while MSA Pairformer effectively encodes evolutionary relatedness, and does so in specific layers, this signal is a subtle optimization rather than a load-bearing pillar for contact prediction accuracy. This work is intended for biologists and model developers seeking to interpret how evolutionary history is represented within pLMs. It provides a framework for evaluating the “phylogenetic operating range” of MSA-based models and highlights the need for future architectures to selectively gauge input signal reliability."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Introduction",
    "text": "Introduction\nThe current trajectory of protein language modeling is dominated by a drive toward massive parameter scaling. Models such as ESM2 (Lin et al., 2023) have achieved strong performance by internalizing the evolutionary statistics of protein families within billions of learned weights. However, as sequence databases expand at a rate that outpaces feasible computational growth, this “memorization” through scaling faces a significant sustainability challenge (Akiyama et al., 2025).\nMSA-based architectures, such as MSA Transformer (Rao et al., 2021) and the more recent MSA Pairformer (Akiyama et al., 2025), represent a pivot toward models that utilize external evolutionary context as input. By extracting information directly from multiple sequence alignments (MSAs) passed as input to the model at runtime, these models shift the burden of knowledge from fixed parameters to the specific sequences provided in the MSA, allowing for high performance with significantly fewer model parameters. Yet, this shift introduces a new architectural hurdle: resolving specific structural signals within a divergent alignment without diluting them through phylogenetic averaging1.\n1 A limitation in MSA-based models whereby sequences are treated as independent of one another, disregarding their evolutionary relatedness to one another. By treating divergent lineages as if they share an identical structure, this process dilutes subfamily-specific signals, like distinct binding interfaces, which can obscure unique evolutionary constraints of the query sequence.2 In MSA Pairformer, each MSA has one sequence-of-interest, denoted as the “query.”MSA Pairformer addresses this via a query-biased attention mechanism, positing that performance is maximized when the model can selectively weight sequences based on their specific evolutionary relevance to the query2. However, evolutionary relevance is vague and lacks a precise metric.\nTo address this ambiguity, we investigate if evolutionary relatedness (measured as patristic distance3 on a phylogenetic tree) serves as a quantifiable correlate of these sequence weights. While the input MSA implicitly encodes phylogenetic history, it remains to be seen if the model actively leverages this biological signal, or if it instead relies on alternative statistical heuristics to build its representations. Distinguishing between these possibilities is critical for determining if the model’s representations are grounded in evolutionary logic as purported, and if this grounding is a causal driver of downstream performance. While the original paper showed that attention weights could separate subfamilies, this was not a fully-fledged characterization of how sequence weights map onto phylogenetic structure, and was demonstrated for just one protein family. This leaves open the question of how accurately the model’s attention mechanism recapitulates evolutionary relatedness and how broadly these patterns hold across diverse protein families.\n3 Patristic distance is the distance between two tips/leaves of a phylogenetic tree, calculated as the sum of branch lengths connecting them. Mathematically, \\(d_{ij} = \\sum_{b \\in P_{ij}} l(b)\\), where \\(P_{ij}\\) is the shortest path through the phylogeny connecting taxa \\(i\\) and \\(j\\), and \\(l(b)\\) is the length of branch \\(b\\).In this work, we explore these questions directly. We begin by reanalyzing the response regulator (RR) protein family case study presented by Akiyama et al. (2025), and then expand our investigation to thousands of MSAs of diverse protein families spanning the tree of life. Our goal is to scope out the anatomy of the model’s learning of evolutionary distance and how it relates to downstream performance."
  },
  {
    "objectID": "index.html#revisiting-the-response-regulator-study",
    "href": "index.html#revisiting-the-response-regulator-study",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Revisiting the response regulator study",
    "text": "Revisiting the response regulator study\nThe response regulator (RR) family was the original paper’s showcase for demonstrating the power of query-biased attention. By constructing a mixed MSA of GerE, LytTR, and OmpR subfamilies, the authors showed MSA Pairformer could successfully identify key structural contacts unique to each subfamily.\nThey illustrated one potential mechanism behind this success by plotting model sequence weights against Hamming distance4, revealing that members of the query’s subfamily were consistently upweighted, leading to an inverse correlation between Hamming distance (a proxy for evolutionary distance) and sequence weight, as shown below.\n4 Hamming distance is the number of positions at which two sequences of equal length differ.\n\n\nFigure 4B from Akiyama et al. (2025). Original caption: “Median sequence weight across the layers of the model versus Hamming distance to the query sequence. Top panels show distribution of sequence attention weights for subfamily members (red) and non-subfamily sequences (grey). The grey dotted line indicates weights used for uniform sequence attention and the red dotted line indicates weight assigned to the query sequence.”\n\n\nWe can build on this analysis by replacing the proxy of Hamming distance with a formal phylogenetic tree inferred from the input MSA, which would allow us to ask a more nuanced question: Do the model’s sequence weights recapitulate the continuous evolutionary distances among individual sequences, or do they more closely resemble a binary distinction between “in-group” and “out-group”?\nTo ground our phylogenetic analysis in the paper’s original findings, we start by replicating the dataset. Following the protocol laid out by Akiyama et al. (2025), we begin by downloading the full PFAM alignments for the GerE, LytTR, and OmpR subfamilies, combining them, and then sampling a final set of 4096 sequences to match the dataset used in the study.\n\n\n\n\n\n\nNoteReproducing the response regulator MSAs\n\n\n\nAkiyama et al. (2025) qualitatively describe how to reproduce the response regulator MSAs, however these details are insufficient for exact replication. The code below is our attempted reproduction, and we find these MSAs yield similar, yet not identical, sequence weight statistics.\n\n\nResponse regulator MSA code\nfrom collections import Counter\nfrom pathlib import Path\n\nimport pandas as pd\nfrom MSA_Pairformer.dataset import MSA\n\nfrom analysis.pfam import download_and_process_response_regulator_msa\n\nresponse_regulator_dir = Path(\"./data/response_regulators\")\nresponse_regulator_dir.mkdir(parents=True, exist_ok=True)\n\nrr_msas: dict[str, MSA] = {}\nrr_queries = {\"1NXS\": \"OmpR\", \"4CBV\": \"LytTR\", \"4E7P\": \"GerE\"}\nfor query in rr_queries:\n    msa_path = response_regulator_dir / f\"PF00072.final_{query}.a3m\"\n\n    if not msa_path.exists():\n        download_and_process_response_regulator_msa(\n            output_dir=response_regulator_dir,\n            subset_size=4096,\n        )\n\n    rr_msas[query] = MSA(msa_file_path=msa_path, diverse_select_method=\"none\")\n\nexample_msa = rr_msas[query]\nmembership_path = response_regulator_dir / \"membership.txt\"\ntarget_to_subfamily = (\n    pd.read_csv(membership_path, sep=\"\\t\").set_index(\"record_id\")[\"subfamily\"].to_dict()\n)\n\nprint(f\"MSA has {len(example_msa.ids_l)} sequences:\")\n\nsubfamily_member_count = Counter()\nfor sequence in example_msa.ids_l:\n    subfamily_member_count[target_to_subfamily[sequence]] += 1\n\nfor subfamily, count in subfamily_member_count.items():\n    print(f\"  - {subfamily} sequences: {count}\")\n\n\nMSA has 4096 sequences:\n  - GerE sequences: 2545\n  - LytTR sequences: 1087\n  - OmpR sequences: 464\n\n\n\n\nTo infer a tree for this MSA, we use FastTree, a rapid method for approximating maximum-likelihood phylogenies suitable for trees of this size (Price et al., 2009).\n\n\n\n\n\n\nNoteAccuracy concerns using FastTree\n\n\n\nFastTree’s heuristic-based approach to maximum-likelihood inference can sacrifice accuracy compared to more exhaustive methods. We address these concerns in detail in the section, “Is FastTree accurate enough?”, where we benchmark our results against IQ-TREE to quantify the impact of tree inference quality on our findings.\n\n\n\n\nTree inference code\nfrom analysis.tree import read_newick, run_fasttree\n\nfasttree_path = response_regulator_dir / \"PF00072.final.fasttree.newick\"\nfasttree_log_path = response_regulator_dir / \"PF00072.final.fasttree.log\"\nmsa_for_tree = response_regulator_dir / \"PF00072.final.fasta\"\nif not fasttree_path.exists():\n    run_fasttree(msa_for_tree, fasttree_path, log_file=fasttree_log_path)\n\nrr_tree = read_newick(fasttree_path)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this MSA, FastTree ends up using the Jones-Taylor-Thornton (JTT) evolutionary model with CAT approximation (20 rate categories). If you’re following along at home, you can check out the full logs at data/response_regulators/PF00072.final.fasttree.log.\n\n\nVisualizing this tree provides our first direct look at the evolutionary structure of the data.\n\n\nTree visualization code\nimport arcadia_pycolor as apc\n\nfrom analysis.plotting import tree_style_with_categorical_annotation\nfrom analysis.tree import read_newick, subset_tree\n\nquery_colors = {\n    \"4E7P\": apc.aster,\n    \"1NXS\": apc.candy,\n    \"4CBV\": apc.seaweed,\n}\nsubfamily_colors = {rr_queries[k]: v for k, v in query_colors.items()}\n\ntree_style = tree_style_with_categorical_annotation(\n    categories=target_to_subfamily,\n    highlight=list(rr_queries),\n    color_map=subfamily_colors,\n)\nvisualized_tree = subset_tree(tree=rr_tree, n=100, force_include=list(rr_queries), seed=42)\nvisualized_tree.render(\"%%inline\", tree_style=tree_style, dpi=300)\n\n\n\n\n\n\n\n\nFigure 1: A randomly selected subset of the response regulator family illustrating the tree structure. The RCSB structure ID is labelled for the query sequence of each subfamily. Leaves are colored according to subfamily (⬤ GerE, ⬤ OmpR, ⬤ LytTR).\n\n\n\n\n\nWe expect to see largely monophyletic clades corresponding to the three subfamilies, which is more or less what we observe in Figure 1. Discrepancies from this expectation highlight the importance of clade grouping based on explicit tree inference, rather than relying on PFAM domains.\nUsing the phylogeny as our objective target variable, we assess whether the model’s query-biased sequence weights successfully capture evolutionary relatedness. To do this, we’ll run an MSA Pairformer inference three separate times. Each run will use a different subfamily representative as the query, yielding three distinct, query-biased sets of sequence weights.\n\n\n\n\n\n\nNoteRunning MSA Pairformer…\n\n\n\nIn order to reproduce this step of the workflow, you’ll need a GPU with at least 40GB of GPU VRAM. We don’t assume you have that hardware handy, so we’ve stored pre-computed inference results (data/response_regulators/inference_results.pt), and by default, the code below will load these inference results rather than re-computing. If you have the necessary hardware and want to re-compute the inference, delete this file prior to running the cell below, and the file will be regenerated.\n\n\nMSA inference code\nfrom typing import Any\n\nimport torch\n\nfrom analysis.pairformer import run_inference\n\ninference_results_path = response_regulator_dir / \"inference_results.pt\"\nif inference_results_path.exists():\n    rr_inference_results = torch.load(inference_results_path, weights_only=True)\nelse:\n    rr_inference_results: dict[str, dict[str, Any]] = {}\n    for query in rr_queries:\n        rr_inference_results[query] = run_inference(\n            rr_msas[query], return_seq_weights=True, query_only=True\n        )\n\n    torch.save(rr_inference_results, inference_results_path)\n\n\n\n\nWe now have our two key components: a phylogenetic tree for the RR family (our objective target variable) and the model’s sequence weights relative to each of the three subfamily queries. Before diving into a formal statistical analysis, let’s build an intuition for how the model’s attention relates to tree structure by visualizing weights directly onto the tree.\nFor each of the three queries, let’s center our view on a small subset of the full MSA (for ease of visualization) and color at each leaf the median (across layers) sequence weight it received from the model. If the model is capturing evolutionary relatedness, we’d expect a gradient of sequence weight that follows the tree’s branches away from the query.\n\nCode\nfrom IPython.display import display\n\nfrom analysis.data import get_sequence_weight_data\nfrom analysis.plotting import tree_style_with_scalar_annotation\nfrom analysis.tree import (\n    sort_tree_by_reference,\n    subset_tree_around_reference,\n)\n\nrr_data_dict = dict(\n    query=[],\n    target=[],\n    median_weight=[],\n)\n\nfor query in rr_queries:\n    msa = rr_msas[query]\n    targets = msa.ids_l\n\n    weights = get_sequence_weight_data(rr_inference_results[query])\n\n    # For each layer, sequence weights sum to 1. Scaling by number of\n    # sequences yields a scale where 1 implies uniform weighting.\n    weights *= weights.size(0)\n\n    median_weights = torch.median(weights, dim=1).values\n\n    rr_data_dict[\"query\"].extend([query] * len(targets))\n    rr_data_dict[\"target\"].extend(targets)\n    rr_data_dict[\"median_weight\"].extend(median_weights.tolist())\n\nresponse_regulator_df = pd.DataFrame(rr_data_dict)\n\ntree_images = []\nqueries_list = response_regulator_df[\"query\"].unique()\nfor query in queries_list:\n    color = query_colors[query]\n    specific_layer = \"median_weight\"\n    specific_layer_weights = (\n        response_regulator_df.loc[\n            (response_regulator_df[\"query\"] == query)\n            & (response_regulator_df[\"query\"] != response_regulator_df[\"target\"]),\n            [specific_layer, \"target\"],\n        ]\n        .set_index(\"target\")[specific_layer]\n        .to_dict()\n    )\n\n    gradient = apc.Gradient.from_dict(\n        \"gradient\",\n        {\"1\": \"#EEEEEE\", \"2\": \"#EEEEEE\", \"3\": color, \"4\": color},\n        values=[0.0, 0.0, 0.75, 1.0],\n    )\n    tree_style = tree_style_with_scalar_annotation(\n        specific_layer_weights, gradient, highlight=[query]\n    )\n    visualized_tree = sort_tree_by_reference(\n        subset_tree_around_reference(tree=rr_tree, n=100, reference=query, bias_power=0.8, seed=42),\n        query,\n    )\n    tree_images.append(visualized_tree.render(\"%%inline\", tree_style=tree_style, dpi=300))\n\ndisplay(*tree_images)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Median sequence weights with respect to 1NXS (OmpR).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Median sequence weights with respect to 4CBV (LytTR).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Median sequence weights with respect to 4E7P (GerE).\n\n\n\n\n\n\n\nFigure 2: Unrooted trees for each query, where each leaf is colored according to the median sequence weight it received from the model. Darker nodes signify sequences receiving high levels of attention (upweighting), while lighter nodes signify sequences receiving low levels of attention (downweighting). Each tree is subset to 100 sequences sampled from the full tree (includes all subfamilies). To see whether we observe a gradient, sequences were sampled with a probability inversely proportional to their phylogenetic rank distance from the query, raised to a power of 0.8, which combine to give slight preference for selecting sequences phylogenetically similar to the query while still sampling across the entire phylogeny.\n\n\n\nEncouragingly, Figure 2 provides an intuitive picture: weights with respect to 1NXS and 4CBV visually correlate with distance from the query, suggesting the model’s attention mechanism prioritizes evolutionary relatedness. However, the picture is less clear for 4E7P, which invites a quantitative test.\nTo formalize this observation, let’s extract the patristic distance between the queries and all the other sequences in the MSA, then compare this to the model’s median sequence weights. As in Akiyama et al. (2025), we’ll normalize weights by the number of sequences, so a value of 1 represents the uniform weighting baseline and a value greater than 1 indicates upweighting. And on the suspicion that this median value might smooth over layer-specific complexity, let’s also store the individual weights from each layer to leave room for a more granular, layer-by-layer analysis.\n\n\nCode\nfrom scipy.stats import linregress\n\nfrom analysis.data import get_sequence_weight_data\nfrom analysis.tree import get_patristic_distance\n\nrr_data_dict = dict(\n    query=[],\n    target_subfamily=[],\n    target=[],\n    patristic_distance=[],\n    median_weight=[],\n)\n\nnum_layers = 22\nfor layer_idx in range(num_layers):\n    rr_data_dict[f\"layer_{layer_idx}_weight\"] = []\n\nfor query in rr_queries:\n    msa = rr_msas[query]\n    targets = msa.ids_l\n\n    patristic_distances = get_patristic_distance(rr_tree, query)\n    patristic_distances = patristic_distances[targets]\n\n    weights = get_sequence_weight_data(rr_inference_results[query])\n\n    # For each layer, sequence weights sum to 1. Scaling by number of\n    # sequences yields a scale where 1 implies uniform weighting.\n    weights *= weights.size(0)\n\n    median_weights = torch.median(weights, dim=1).values\n\n    for layer_idx in range(num_layers):\n        rr_data_dict[f\"layer_{layer_idx}_weight\"].extend(weights[:, layer_idx].tolist())\n\n    rr_data_dict[\"query\"].extend([query] * len(targets))\n    rr_data_dict[\"target_subfamily\"].extend([target_to_subfamily[target] for target in targets])\n    rr_data_dict[\"target\"].extend(targets)\n    rr_data_dict[\"median_weight\"].extend(median_weights.tolist())\n    rr_data_dict[\"patristic_distance\"].extend(patristic_distances.tolist())\n\nresponse_regulator_df = pd.DataFrame(rr_data_dict)\nresponse_regulator_df = response_regulator_df.query(\"query != target\").reset_index(drop=True)\nresponse_regulator_df\n\n\nWe’ll analyze the relationship between sequence weights and patristic distance with a simple linear regression. We’ll frame the problem to directly assess the explanatory power of the model’s sequence weights: how well do they explain the patristic distance to the query?\nFor each query \\(q\\) and each target sequence \\(i\\) in the MSA, let’s define our model as:\n\\[\nd_{i} = \\beta_1^{(l)} w_{i}^{(l)} + \\beta_0^{(l)}\n\\tag{1}\\]\nwhere \\(d_{i}\\) is the patristic distance from the query \\(q\\) to the target sequence \\(i\\), \\(w_{i}^{(l)}\\) is the normalized sequence weight assigned to sequence \\(i\\) by a specific layer \\(l\\), and \\(\\beta_1^{(l)}\\) and \\(\\beta_0^{(l)}\\) are the slope and intercept for the regression at layer \\(l\\).\nLet’s perform this regression independently for each of the three queries. For each query, we’ll calculate the fit using the median weight across all layers and also for each of the 22 layers individually.\nWe’ll use the coefficient of determination (\\(R^2\\)) as the key statistic to measure the proportion of the variance in patristic distance that is explainable from the sequence weights. The following code calculates these regression statistics and generates an interactive plot to explore the relationships.\n\n\nCode\nimport arcadia_pycolor as apc\n\nfrom analysis.plotting import interactive_layer_weight_plot\n\nregression_data = dict(\n    query=[],\n    layer=[],\n    r_squared=[],\n    p_value=[],\n    slope=[],\n    intercept=[],\n)\n\nfor query in queries_list:\n    query_data = response_regulator_df[response_regulator_df[\"query\"] == query]\n    y = query_data[\"patristic_distance\"].values\n    x = query_data[\"median_weight\"].values\n    result = linregress(x, y)\n    regression_data[\"query\"].append(query)\n    regression_data[\"layer\"].append(\"median\")\n    regression_data[\"r_squared\"].append(result.rvalue**2)\n    regression_data[\"p_value\"].append(result.pvalue)\n    regression_data[\"slope\"].append(result.slope)\n    regression_data[\"intercept\"].append(result.intercept)\n\n    for layer_idx in range(num_layers):\n        weight_col = f\"layer_{layer_idx}_weight\"\n        x = query_data[weight_col].values\n        result = linregress(x, y)\n        regression_data[\"query\"].append(query)\n        regression_data[\"layer\"].append(layer_idx)\n        regression_data[\"r_squared\"].append(result.rvalue**2)\n        regression_data[\"p_value\"].append(result.pvalue)\n        regression_data[\"slope\"].append(result.slope)\n        regression_data[\"intercept\"].append(result.intercept)\n\nrr_regression_df = pd.DataFrame(regression_data)\nrr_regression_df\n\napc.plotly.setup()\ninteractive_layer_weight_plot(response_regulator_df, rr_regression_df, rr_queries, subfamily_colors)\n\n\n\n\n                            \n                                            \n\n\nFigure 3: An interactive display illustrating sequence weight versus patristic distance for each MSA member to the query. Each subplot represents the sequence weights relative to a different query. The dropdown controls which layer the sequence weights are from. By default, the median sequence weights across all layers are visualized. Black lines indicate the lines of best fit.\n\n\n\n\nWhen viewing the median sequence weights in Figure 3, we observe a negative correlation with patristic distance across all three subfamilies. This provides quantitative support for the original paper’s central claim: on average, the model effectively learns to upweight evolutionarily closer sequences and downweight more distant ones.\nHowever, the layer-by-layer analysis uncovers a more nuanced and specialized division of labor. The strength, and even the direction, of this correlation varies with network depth. Some layers, such as layer 11, act as powerful phylogenetic distance filters—selectively upweighting sequences evolutionarily close to the query while suppressing distant ones. They exhibit a strong negative correlation (\\(R^2 &gt; 0.6\\) in some cases), sharply penalizing sequences as their evolutionary distance from the query increases. Other layers show weak or even positive correlations (e.g., layer 12), although these layers are a minority. We also observe distinct behavior in layers 18 and 20, which yield heavy-tailed distributions characterized by a few outliers receiving exceptionally high weights.\nAlthough the model was never trained to predict phylogenetic distance, the sequence weights produced during inference nevertheless encode a substantial amount of information for doing so, with some layers appearing to be especially information rich."
  },
  {
    "objectID": "index.html#a-survey-across-the-tree-of-life",
    "href": "index.html#a-survey-across-the-tree-of-life",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "A survey across the tree of life",
    "text": "A survey across the tree of life\nOur analysis shows that median sequence weights correlate moderately with phylogenetic distance. More intriguingly, this layer-by-layer view has given us a peek behind the curtain, revealing a complex division of labor in how the model’s attention mechanism captures evolutionary relatedness through the query-biased outer product.\nWe want to further characterize MSA Pairformer’s understanding of evolutionary relationships more broadly, so let’s expand our analysis to thousands of diverse protein families.\nTo do this, we turn to the OpenProteinSet (Ahdritz et al., 2023), a massive public database of protein alignments. This resource, derived from UniClust30 and hosted on AWS, provides the scale we need to move beyond our single case study.\nInferring phylogenetic trees for all ~270,000 UniClust30 MSAs in the collection would require roughly 10 times the amount of patience most people possess. Furthermore, some of these MSAs would be unsuitable for our analysis for one reason or another. So to whittle this down to a more digestible size, we’ll create the following procedure.\n\n\n\n\n\n\nNoteMSA pre-processing workflow\n\n\n\nFirst, randomly select 20,000 MSAs from the UniClust30 collection. Then, for each of the 20,000 MSAs, apply the following procedure:\n\nSelect a diverse subset of up to 1024 sequences from the MSA\n\nDo this with the MSA Pairformer API, which implements the sampling procedure introduced in MSA Transformer (Rao et al., 2021)\n\nApply several filters that if the MSA does not pass, it is discarded:\n\nToo shallow (fewer than 200 sequences), posing overfitting issues for downstream modelling (more on that later).\nToo long (over 1024 residues), posing computational constraints.\nContains duplicate sequence identifiers.\n\n\n\n\nImplementation of the workflow\nimport random\n\nfrom analysis.open_protein_set import fetch_all_ids, fetch_msas\nfrom analysis.sequence import write_processed_msa\nfrom analysis.utils import progress\n\nuniclust30_dir = Path(\"data\") / \"uniclust30\"\nuniclust30_dir.mkdir(parents=True, exist_ok=True)\n\nuniclust30_msa_dir = uniclust30_dir / \"msas\"\ncomplete_marker = uniclust30_msa_dir / \".complete\"\n\nif not complete_marker.exists():\n    msa_ids_path = uniclust30_dir / \"ids\"\n    msa_ids = fetch_all_ids(cache_file=msa_ids_path)\n\n    random.seed(42)\n    msa_ids_subset = random.sample(msa_ids, k=20000)\n\n    uniclust30_raw_msa_dir = uniclust30_dir / \"raw_msas\"\n    uniclust30_raw_msa_dir.mkdir(exist_ok=True)\n\n    raw_msa_paths = fetch_msas(msa_ids_subset, db_dir=uniclust30_raw_msa_dir)\n\n    max_seq_length = 1024\n    min_sequences = 200\n\n    uniclust30_msa_dir.mkdir(exist_ok=True)\n\n    skipped_file = uniclust30_dir / \"skipped_ids\"\n    if skipped_file.exists():\n        skipped_set = set(skipped_file.read_text().strip().split(\"\\n\"))\n    else:\n        skipped_set = set()\n\n    for id, raw_msa_path in progress(raw_msa_paths.items(), desc=\"Processing MSAs\"):\n        msa_path = uniclust30_msa_dir / f\"{id}.a3m\"\n\n        if msa_path.exists():\n            skipped_set.add(id)\n            continue\n\n        if id in skipped_set:\n            continue\n\n        msa = MSA(\n            raw_msa_path,\n            max_seqs=1024,\n            max_length=max_seq_length + 1,\n            diverse_select_method=\"hhfilter\",\n            secondary_filter_method=\"greedy\",\n        )\n\n        # Skip MSAs containing duplicate deflines. This likely occurs when multi-domain proteins\n        # generate multiple alignment hits. Duplicate names would cause tree construction to fail.\n        deflines = [msa.ids_l[idx] for idx in msa.select_diverse_indices]\n        if len(set(deflines)) != len(deflines):\n            skipped_set.add(id)\n            continue\n\n        # We simplify verbose deflines from format tr|A0A1V5V6X5|LONG_SUFFIX to just A0A1V5V6X5.\n        # In rare cases (~0.5% of MSAs), simplification creates duplicates when both a consensus\n        # sequence (tr|ID|ID_consensus) and its non-consensus counterpart (tr|ID|ID_SPECIES)\n        # are present in the alignment. Rather than handle this edge case, we skip these MSAs.\n        simplified_deflines = [defline.split(\"|\")[1] for defline in deflines]\n        if len(set(simplified_deflines)) != len(simplified_deflines):\n            skipped_set.add(id)\n            continue\n\n        # Skip MSAs exceeding maximum sequence length due to memory constraints\n        if msa.select_diverse_msa.shape[1] &gt; max_seq_length:\n            skipped_set.add(id)\n            continue\n\n        # Skip MSAs with too few sequences to avoid overfitting when modeling\n        # patristic distance with all 22 sequence weights.\n        if msa.select_diverse_msa.shape[0] &lt; min_sequences:\n            skipped_set.add(id)\n            continue\n\n        # Write processed MSA to A3M format\n        write_processed_msa(msa, msa_path, format=\"a3m\", simplify_ids=True)\n\n    _ = skipped_file.write_text(\"\\n\".join(skipped_set) + \"\\n\")\n    complete_marker.touch()\n\nmsas = {}\nfor msa_path in sorted(uniclust30_msa_dir.glob(\"*.a3m\")):\n    msas[msa_path.stem] = MSA(msa_path, diverse_select_method=\"none\")\n\nprint(f\"Final MSA count: {len(msas)}\")\n\n\n\n\nLike before, we calculate trees and sequence weights for each MSA.\n\n\nCalculating phylogenies\nimport asyncio\nimport os\n\nfrom analysis.tree import run_fasttree_async\n\nuniclust30_tree_dir = uniclust30_dir / \"trees\"\nuniclust30_tree_dir.mkdir(exist_ok=True)\n\njobs = []\nsemaphore = asyncio.Semaphore(os.cpu_count() - 1)\nfor a3m_path in uniclust30_msa_dir.glob(\"*.a3m\"):\n    fasttree_path = uniclust30_tree_dir / f\"{a3m_path.stem}.fasttree.newick\"\n    log_path = uniclust30_tree_dir / f\"{a3m_path.stem}.fasttree.log\"\n    if fasttree_path.exists():\n        continue\n\n    jobs.append(run_fasttree_async(a3m_path, fasttree_path, log_path, semaphore))\n\n_ = await asyncio.gather(*jobs)\n\ntrees = {}\nfor tree_path in progress(\n    sorted(uniclust30_tree_dir.glob(\"*.fasttree.newick\")),\n    desc=\"Loading trees\",\n):\n    id = tree_path.name.split(\".\")[0]\n    tree = read_newick(tree_path)\n    trees[id] = tree\n\n\n\n\nCalculating sequence weights\nfrom analysis.pairformer import calculate_sequence_weights\n\nseq_weights_path = uniclust30_dir / \"seq_weights.pt\"\n\n# When running on Modal, calculate_sequence_weights serializes MSAs to send to remote GPU workers.\n# Serializing all MSAs at once exceeds Modal's serialization limits, so we batch into groups.\n# This constraint is specific to Modal's RPC layer. This batching choice is unnecessary\n# yet harmless for local execution.\nseq_weights = {}\nbatch_size = 250\n\n_msa_items = list(msas.items())\n\nif seq_weights_path.exists():\n    seq_weights = torch.load(seq_weights_path, weights_only=True)\nelse:\n    for batch_start in progress(\n        range(0, len(_msa_items), batch_size), desc=\"Calculating sequence weights\"\n    ):\n        _batch_msas = dict(_msa_items[batch_start : batch_start + batch_size])\n        _batch_weights = calculate_sequence_weights(_batch_msas)\n        seq_weights.update(_batch_weights)\n\n    torch.save(seq_weights, seq_weights_path)\n\n\n\n\n\n\n\n\nNoteFor those following at home\n\n\n\n\n\nAfter running the above computations, you’ll have the primary data in the following directories:\n\nMSAs: data/uniclust30/msas\ntrees: data/uniclust30/trees\nsequence weights: data/uniclust30/seq_weights.pt\n\nThese data could be a prime launch point for followup studies."
  },
  {
    "objectID": "index.html#explanatory-model-across-all-layers",
    "href": "index.html#explanatory-model-across-all-layers",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Explanatory model across all layers",
    "text": "Explanatory model across all layers\nIn Figure 3, we performed separate linear regressions for each layer, illustrating the explanatory power of each layer in isolation. Now, with thousands of MSAs, we can graduate to a more comprehensive question. Instead of asking how well a single layer predicts evolutionary distance, we ask:\n\n\n\n\n\n\nTipKey Question\n\n\n\nHow well do sequence weights from all 22 layers, when used jointly, predict phylogenetic distance?\n\n\nIn posing this question, we must be clear about our goal: we’re interested in assessing explanatory power of these sequence weights, not predictive power. In other words, we’re using these regressions as a way to quantify the in-sample5 explanatory power of the model’s complete set of sequence weights.\n5 “In-sample” analysis evaluates a model using the same dataset that was used to fit its parameters. The goal is to measure goodness-of-fit, i.e., how well the model describes the data it was built from. This contrasts with “out-of-sample” analysis, which would use a separate, “held-out” dataset to assess predictive power and the model’s ability to generalize.Mathematically, we define a weight vector \\(\\mathbf{w}_i\\) for each sequence \\(i\\), which is composed of the weights from all \\(L\\) layers. Our model then finds the single coefficient vector \\(\\boldsymbol{\\beta}\\) that best maps these weights to the patristic distance \\(d_i\\):\n\\[\n\\mathbf{w}_i = \\begin{bmatrix} w_i^{(1)} \\\\ w_i^{(2)} \\\\ \\vdots \\\\ w_i^{(L)} \\end{bmatrix} \\quad \\text{and} \\quad d_i = \\boldsymbol{\\beta}^T \\mathbf{w}_i + \\beta_0\n\\]\nSince we’re using 22 predictors \\((L=22)\\) and our MSAs have varying numbers of sequences \\((N)\\), we risk overfitting when \\(N\\) is low. We mitigate this risk by filtering MSAs with fewer than 200 sequences, so that our shallowest MSAs yield an observation to parameter ratio of around 10:1. Furthermore, we score goodness-of-fit using adjusted \\(R^2\\), \\(R^2_\\text{adj}\\), which penalizes scores for MSAs with low depth6.\n6 Adjusted \\(R^2\\) is defined as \\(R^2_{\\text{adj}} = 1 - (1 - R^2)\\frac{n - 1}{n - p - 1}\\), which corrects \\(R^2\\) for the number of predictors \\(p\\) and sample size \\(n\\), thereby penalizing overparameterized models and providing an unbiased estimate of explained variance. Learn more here.\nFitting the model\nLet’s go ahead and fit this model to each MSA in our collection.\n\n\nRunning a linear regression on each MSA\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed\nfrom scipy.stats import spearmanr\n\nfrom analysis.regression import regress_and_analyze_features\n\n\ndef process_msa(\n    query: str, query_length: int, dist_to_query: np.ndarray, weights: torch.Tensor\n) -&gt; dict[str, Any]:\n    data = {}\n\n    # Regress the sequence weights against patristic distance.\n    # Perform an ANOVA (type III) to establish explanatory importance\n    # of each layer's sequence weights.\n    model, anova_table = regress_and_analyze_features(weights, dist_to_query)\n\n    y_pred = model.fittedvalues\n    y_actual = model.model.endog\n    spearman_statistic, _ = spearmanr(y_pred, y_actual)\n\n    data[\"Query\"] = query\n    data[\"MSA Depth\"] = len(dist_to_query)\n    data[\"MSA Length\"] = query_length\n    data[\"R2\"] = model.rsquared\n    data[\"Adjusted R2\"] = model.rsquared_adj\n    data[\"Spearman\"] = spearman_statistic\n    data.update(anova_table[\"percent_sum_sq\"].to_dict())\n\n    return data\n\n\njobs = []\nfor query in trees.keys():\n    msa = msas[query]\n    tree = trees[query]\n    weights = seq_weights[query]\n\n    query_len = msa.select_diverse_msa.shape[1]\n\n    size = len(tree.get_leaf_names())\n    if size &lt; 200:\n        continue\n\n    dist_to_query = get_patristic_distance(tree, query)[msa.ids_l].values\n    jobs.append(delayed(process_msa)(query, query_len, dist_to_query, weights))\n\nresults_df = pd.DataFrame(Parallel(-1)(jobs))\nresults_df.iloc[:, :6]\n\n\n\n\n\n  \n    \n      \n      Query\n      MSA Depth\n      MSA Length\n      R2\n      Adjusted R2\n      Spearman\n    \n  \n  \n    \n      0\n      A0A009GC83\n      642\n      346\n      0.948489\n      0.946659\n      0.904931\n    \n    \n      1\n      A0A011NYS2\n      763\n      222\n      0.692066\n      0.682911\n      0.818845\n    \n    \n      2\n      A0A014KW24\n      249\n      245\n      0.572092\n      0.530437\n      0.688290\n    \n    \n      3\n      A0A014N001\n      456\n      277\n      0.854116\n      0.846704\n      0.813512\n    \n    \n      4\n      A0A015J5C0\n      406\n      282\n      0.718079\n      0.701885\n      0.674596\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5408\n      X8DTN4\n      359\n      310\n      0.838839\n      0.828287\n      0.903252\n    \n    \n      5409\n      X8HRM7\n      286\n      264\n      0.891448\n      0.882367\n      0.929148\n    \n    \n      5410\n      X8JJD3\n      272\n      284\n      0.897150\n      0.888063\n      0.929121\n    \n    \n      5411\n      Y0KI99\n      1017\n      285\n      0.904341\n      0.902224\n      0.905972\n    \n    \n      5412\n      Z4X668\n      1024\n      341\n      0.698371\n      0.691742\n      0.796673\n    \n  \n\n5413 rows × 6 columns\n\n\n\n\n\n\n\n\n\nNotePlotting the residuals of a few examples\n\n\n\nBelow are some examples of good and bad fit.\n\n\nCode\nimport arcadia_pycolor as apc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom analysis.plotting import (\n    tree_style_with_highlights,\n)\nfrom analysis.regression import regress_and_analyze_features\nfrom analysis.tree import get_patristic_distance\n\napc.mpl.setup()\n\n\ndef compute_regression_residuals(tree, msa, weights, query):\n    tree = tree.copy()\n    dist_to_query_series = get_patristic_distance(tree, query)[msa.ids_l]\n\n    valid_ids = dist_to_query_series[dist_to_query_series &gt; 0].index.tolist()\n    dist_to_query_filtered = dist_to_query_series[valid_ids].values\n\n    weights = weights.clone()[1:, :]\n    model, _ = regress_and_analyze_features(weights, dist_to_query_filtered)\n\n    y_pred = model.fittedvalues\n    y_actual = model.model.endog\n    residuals = np.abs(y_actual - y_pred)\n\n    residuals_dict = {seq_id: res for seq_id, res in zip(valid_ids, residuals, strict=True)}\n\n    return model, residuals_dict, y_actual, y_pred, valid_ids\n\n\ndef plot_tree_and_expected_versus_actual(tree, msa, weights, query, subset=0):\n    tree = tree.copy()\n    model, residuals_dict, y_actual, y_pred, valid_ids = compute_regression_residuals(\n        tree, msa, weights, query\n    )\n\n    if subset &gt; 0:\n        rendered_tree = subset_tree(tree, force_include=[query], n=subset)\n    else:\n        rendered_tree = tree\n    style = tree_style_with_highlights(highlight=[query])\n    style.scale = 120\n    style.margin_bottom = 100\n    tree_render = rendered_tree.render(\"%%inline\", tree_style=style)\n\n    fig, ax = plt.subplots()\n    min_val = min(y_actual.min(), y_pred.min())\n    max_val = max(y_actual.max(), y_pred.max())\n    ax.plot([min_val, max_val], [min_val, max_val], \"k-\", lw=1)\n\n    ax.scatter(y_actual, y_pred, alpha=0.7, s=50, c=apc.marine.hex_code)\n\n    ax.set_xlabel(\"Actual\", fontsize=16)\n    ax.set_ylabel(\"Predicted\", fontsize=16)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n\n    apc.mpl.style_plot(ax, monospaced_axes=\"both\")\n\n    annotation_text = f\"R² Adjusted: {model.rsquared_adj:.3f}\"\n    ax.text(\n        0.05,\n        0.95,\n        annotation_text,\n        transform=ax.transAxes,\n        fontsize=14,\n        fontfamily=apc.mpl.MONOSPACE_FONT,\n        verticalalignment=\"top\",\n    )\n    return tree_render, fig\n\n\npop_quantile = [0.20, 0.5, 0.95]\n_subset = results_df[(results_df[\"MSA Depth\"] &gt; 200) & (results_df[\"MSA Depth\"] &lt; 250)]\nrank_selection = [int(_subset.shape[0] * sel) for sel in pop_quantile]\nthree_examples = _subset.sort_values(by=\"Adjusted R2\").iloc[rank_selection, 0].tolist()\n\nfor example in three_examples:\n    tree = trees[example].copy()\n    msa = msas[example]\n    weights = seq_weights[example]\n\n    tree_render, fig = plot_tree_and_expected_versus_actual(tree, msa, weights, example, subset=100)\n    display(tree_render)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Phylogenetic tree for low R² example (20th percentile)\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicted vs. actual distances for low R² example\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Phylogenetic tree for median R² example (50th percentile)\n\n\n\n\n\n\n\n\n\n\n\n(d) Predicted vs. actual distances for median R² example\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Phylogenetic tree for high R² example (95th percentile)\n\n\n\n\n\n\n\n\n\n\n\n(f) Predicted vs. actual distances for high R² example\n\n\n\n\n\n\n\nFigure 4: Plots of prediction versus actual, alongside the corresponding tree. Examples with similar MSA depth (200-250) were chosen. Trees are subset to 100 randomly sampled nodes.\n\n\n\n\nNotably, mapping the prediction residuals onto the phylogenies revealed no systematic clade-specific bias, indicating that over- and under-estimation errors are distributed uniformly across the trees and do not exhibit clade-specific trends.\n\n\nWhen we consider the model fits in aggregate, an interesting picture emerges.\n\n\nPlotting the results\nfrom analysis.plotting import ridgeline_r2_plot\n\nridgeline_r2_plot(results_df, gradient=apc.gradients.verde.reverse(), gap=0.6)\n\n\n\n\n                            \n                                            \n\n\nFigure 5: Overview of linear regression performance, when binning MSAs by depth (number of sequences). (Left) A barplot showing the number of MSAs found in each bin. (Right) Density plots showing the distribution of \\(R^2_\\text{adj}\\) within each bin. Hovering over each distribution reveals its mean and standard deviation.\n\n\n\n\nFigure 5 provides a high-level overview of MSA Pairformer’s ability to explain phylogenetic distance via the query-biased outer product sequence weights, across thousands of MSAs. The left panel presents a bar chart indicating the distribution of MSA depths7 in our dataset. We observe progressively fewer MSAs in bins of increasing depth, except for the final bin (900-1024 sequences), which occurs due to an artifact of our preprocessing, whereby MSAs with more than 1024 sequences are subset to 1024 sequences. Critically, each bin contains hundreds of MSAs, allowing for a robust statistical comparison across bins. The right panel illustrates the distribution of \\(R^2_\\text{adj}\\) values for each MSA depth bin. Two key observations stand out.\n7 MSA depth is the number of sequences in an MSA.First, the distributions are remarkably stable across all depth bins, suggesting that the model’s joint sequence weights are largely independent of MSA depth. We explore this statistically in the collapsable section below.\nSecond, contrary to the between-group variation, the within-group variation is substantial. Each distribution is broad, spanning a wide range of \\(R^2_\\text{adj}\\) values. This indicates that while average performance is consistent, the model’s ability to explain phylogenetic distance varies dramatically from one MSA to another, even for MSAs of similar size.\n\n\n\n\n\n\nNoteClick to reveal supplementary statistical tests\n\n\n\n\n\nTo statistically validate the visual impression from Figure 5—that MSA depth does not substantially relate to model performance (\\(R^2_{adj}\\))—we perform two tests. First, we use a one-way ANOVA to check for significant differences between the binned MSA depth groups. Second, because binning can introduce arbitrary boundaries, we also run a linear regression to test the direct relationship between \\(R^2_{adj}\\) and MSA depth as a continuous variable.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\n\nbin_edges = [200, 300, 400, 500, 600, 700, 800, 900, 1025]\nbin_labels = [\n    \"200-299\",\n    \"300-399\",\n    \"400-499\",\n    \"500-599\",\n    \"600-699\",\n    \"700-799\",\n    \"800-899\",\n    \"900-1024\",\n]\n\nresults_df[\"MSA Depth Bin\"] = pd.cut(\n    results_df[\"MSA Depth\"], bins=bin_edges, labels=bin_labels, right=False\n)\n\n# One-way ANOVA (binned MSA Depth)\naov_model = ols('Q(\"Adjusted R2\") ~ C(Q(\"MSA Depth Bin\"))', data=results_df).fit()\naov = sm.stats.anova_lm(aov_model, typ=2)\nF = aov.loc['C(Q(\"MSA Depth Bin\"))', \"F\"]\np_aov = aov.loc['C(Q(\"MSA Depth Bin\"))', \"PR(&gt;F)\"]\neta2 = aov.loc['C(Q(\"MSA Depth Bin\"))', \"sum_sq\"] / (\n    aov.loc['C(Q(\"MSA Depth Bin\"))', \"sum_sq\"] + aov.loc[\"Residual\", \"sum_sq\"]\n)\n\nlm = smf.ols('Q(\"Adjusted R2\") ~ Q(\"MSA Depth\")', data=results_df).fit()\nslope = lm.params['Q(\"MSA Depth\")']\nintercept = lm.params[\"Intercept\"]\np_slope = lm.pvalues['Q(\"MSA Depth\")']\nt_slope = lm.tvalues['Q(\"MSA Depth\")']\nr2 = lm.rsquared\nr2_adj = lm.rsquared_adj\nnobs = int(lm.nobs)\n\nfig, ax = plt.subplots()\n\nax.scatter(\n    results_df[\"MSA Depth\"],\n    results_df[\"Adjusted R2\"],\n    alpha=0.4,\n    s=15,\n    color=apc.marine.hex_code,\n    edgecolors=\"none\",\n)\n\npredictions = lm.get_prediction(results_df[[\"MSA Depth\"]])\nprediction_summary = predictions.summary_frame(alpha=0.05)\n\nsort_idx = np.argsort(results_df[\"MSA Depth\"])\nx_sorted = results_df[\"MSA Depth\"].iloc[sort_idx]\ny_pred = prediction_summary[\"mean\"].iloc[sort_idx]\nci_lower = prediction_summary[\"mean_ci_lower\"].iloc[sort_idx]\nci_upper = prediction_summary[\"mean_ci_upper\"].iloc[sort_idx]\n\nax.fill_between(\n    x_sorted,\n    ci_lower,\n    ci_upper,\n    alpha=0.2,\n    color=apc.black.hex_code,\n)\n\nax.plot(\n    x_sorted,\n    y_pred,\n    color=apc.black.hex_code,\n    linewidth=2,\n)\n\nax.set_xlabel(\"MSA depth\")\nax.set_ylabel(\"Adjusted R²\")\napc.mpl.style_plot(ax, monospaced_axes=\"both\")\nplt.show()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"One-way ANOVA on Adjusted R² across MSA Depth Bins\")\nprint(\"=\" * 60)\nprint(f\"F-statistic:        {F:.2f}\")\nprint(f\"p-value:            {p_aov:.3e}\")\nprint(f\"Effect size (η²):   {eta2:.4f}\")\nprint(\"=\" * 60)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Linear Regression: Adjusted R² ~ MSA Depth\")\nprint(\"=\" * 60)\nprint(f\"Intercept:          {intercept:.6g}\")\nprint(f\"Slope:              {slope:.6g}\")\nprint(f\"t-stat (slope):     {t_slope:.4g}\")\nprint(f\"p-value (slope):    {p_slope}\")\nprint(f\"R²:                 {r2:.4f}\")\nprint(f\"Adj. R²:            {r2_adj:.4f}\")\nprint(f\"N:                  {nobs}\")\nprint(\"=\" * 60 + \"\\n\")\n\n\n\n\n\n\n\n\n\n\n============================================================\nOne-way ANOVA on Adjusted R² across MSA Depth Bins\n============================================================\nF-statistic:        12.27\np-value:            1.151e-15\nEffect size (η²):   0.0156\n============================================================\n\n============================================================\nLinear Regression: Adjusted R² ~ MSA Depth\n============================================================\nIntercept:          0.779991\nSlope:              -3.59469e-05\nt-stat (slope):     -5.563\np-value (slope):    2.773342166552301e-08\nR²:                 0.0057\nAdj. R²:            0.0055\nN:                  5413\n============================================================\n\n\n\nWhile both tests return highly significant p-values, the associated effect sizes are negligible (explaining \\(&lt;2\\%\\) and \\(&lt;1\\%\\) of the variance in \\(R^2_\\text{adj}\\), respectively)."
  },
  {
    "objectID": "index.html#msa-pairformers-division-of-labor",
    "href": "index.html#msa-pairformers-division-of-labor",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "MSA Pairformer’s division of labor",
    "text": "MSA Pairformer’s division of labor\nFigure 5 reveals a high-level picture: the joint explanatory power of all 22 layers is robust to MSA depth but highly variable from one MSA to the next. In our initial case study (Figure 3), we saw a “division of labor,” where specific layers (like layer 11) acted as powerful phylogenetic distance filters. Now, with thousands of MSAs, we can (a) further characterize this division of labor and (b) determine how it may vary depending on MSA characteristics.\nTo investigate this, we study the individual explanatory power of each layer, calculated via an ANOVA. This allows us to quantify each layer’s feature importance (as a percentage of the total sum of squares explained) and see how this internal strategy shifts. First, let’s see if the layer importance profile changes with MSA depth in Figure 6.\n\n\nCode\nfrom analysis.plotting import stacked_feature_importance_plot\n\nstacked_feature_importance_plot(\n    results_df,\n    feature_cols=list(range(22)),\n    bin_col=\"MSA Depth\",\n    bin_edges=[200, 300, 400, 500, 600, 700, 800, 900, 1025],\n    bin_display_name=\"MSA depth\",\n    bin_labels=[\n        \"200-299\",\n        \"300-399\",\n        \"400-499\",\n        \"500-599\",\n        \"600-699\",\n        \"700-799\",\n        \"800-899\",\n        \"900-1024\",\n    ],\n    gradient=apc.gradients.verde.reverse(),\n)\n\n\n\n\n                            \n                                            \n\n\nFigure 6: Average feature importance (percent of total sum of squares explained) for each of the 22 layers, binned by MSA depth.\n\n\n\n\nFigure 6 plots the average feature importance profile for each MSA depth bin. The most immediate finding is that these profiles are remarkably consistent across all bins. The model’s “average” strategy for parsing phylogenetic information appears largely independent of MSA size. This figure also confirms our “division of labor” hypothesis on a much larger scale. The contributions are far from uniform:\n\nLayer 11 consistently dominates, single-handedly accounting for ~15% of the explained variance.\nThe first 10 layers all make consistent, nominal contributions.\nThe final layers’ contributions are diminished, particularly those of layers 18 and 20, which we previously noted to exhibit anomalously skewed sequence weight distributions.\n\nThis suggests the model has a “default” strategy for this task. However, this consistent average strategy doesn’t explain the significant variance in performance we saw in Figure 5. What internal strategies are associated with very high (or very low) performance?\nTo answer this, Figure 7 bins the MSAs not by depth, but by their explanatory power (\\(R^2_\\text{adj}\\)).\n\n\nCode\nimport seaborn as sns\n\nfrom analysis.plotting import gradient_from_listed_colormap\n\nflare_gradient = gradient_from_listed_colormap(sns.color_palette(\"flare\", as_cmap=True), \"flare\")\npurple_gradient = gradient_from_listed_colormap(sns.cubehelix_palette(as_cmap=True), \"purple\")\nstacked_feature_importance_plot(\n    results_df,\n    bin_col=\"Adjusted R2\",\n    bin_edges=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    bin_display_name=\"Adjusted R²\",\n    annotation_y_position=0.70,\n    gradient=purple_gradient,\n)\n\n\n\n\n                            \n                                            \n\n\nFigure 7: Average feature importance (percent of total sum of squares explained) for each of the 22 layers, binned by explanatory power \\(R^2_\\text{adj}\\).\n\n\n\n\nCompared to Figure 6, the story in Figure 7 is strikingly different. Instead of stable profiles seen across MSA depths, binning by performance reveals a strong correlation between MSA Pairformer’s sequence weight profile for the MSA and its explanatory power.\nIn cases of low explanatory power (e.g., \\(R^2_\\text{adj} &lt; 0.6\\)), we see MSAs with substantial contributions from layer 7, which for the lowest MSA depth bin matches the normally-dominant layer 11. As performance improves into the intermediate range (\\(R^2_\\text{adj} \\approx 0.6 - 0.8\\)), the strategy reverts to the “default” sequence weight profile we saw in Figure 6.\nMost interestingly, MSAs with the highest explanatory power (\\(R^2_\\text{adj} &gt; 0.8\\)) have disproportionately high contributions from layer 13, and down-weighted importance of layer 11. The relative importance of layer 13 grows steadily with increasing explanatory power, reaching up to 15% in the highest performing bin.\nThese patterns demonstrate that the “division of labor” is not fixed. The model responds to different MSAs by producing different sequence weight profiles. We find these distinct internal strategies, in turn, associate strongly with different levels of success in explaining phylogenetic distance."
  },
  {
    "objectID": "index.html#some-msas-are-phylogenetically-rich-others-poor",
    "href": "index.html#some-msas-are-phylogenetically-rich-others-poor",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Some MSAs are phylogenetically rich, others poor",
    "text": "Some MSAs are phylogenetically rich, others poor\nWhile we have established that MSA Pairformer uses a specific “division of labor” to encode evolutionary distance, the substantial variance in \\(R^2_{\\text{adj}}\\) across protein families remains unexplained. When the evolutionary distances are poorly predicted by sequence weights, is it an indication that the sequence weights themselves encode insufficient evolutionary information, or that the signal in the MSA is itself too weak or ambiguous to be recovered by both the sequence weights and the tree inference method?\n\nIs FastTree accurate enough?\nOur analysis thus far relies on FastTree as the target for our regression. FastTree implements a very efficient algorithm perfectly suited for inferring thousands of trees, though this speed comes at the cost of accuracy. Although IQ-TREE (Minh et al., 2020) is a standard for high-accuracy inference, running it across all our trees is not practical. To ensure our FastTree results are sufficiently accurate, let’s test a subset of MSAs and compare the patristic distances yielded by both methods.\nWe’ll select for smaller MSAs to lower the runtime and use IQ-TREE’s implementation of ModelFinder (Kalyaanamoorthy et al., 2017) for automatic model selection.\n\n\nInferring small subset of trees with IQ-TREE\nfrom analysis.tree import run_iqtree_async\n\nnum_iqtrees = 10\nqueries_for_iqtree = sorted(msas, key=lambda k: msas[k].select_diverse_msa.size)[:num_iqtrees]\n\nuniclust30_iqtree_dir = uniclust30_dir / \"iqtrees\"\nuniclust30_iqtree_dir.mkdir(exist_ok=True)\n\njobs = []\nsemaphore = asyncio.Semaphore(os.cpu_count() - 1)\nfor query in queries_for_iqtree:\n    msa_path = uniclust30_msa_dir / f\"{query}.a3m\"\n    output_path = uniclust30_iqtree_dir / f\"{query}.iqtree.newick\"\n    log_path = uniclust30_iqtree_dir / f\"{query}.iqtree.log\"\n    if not output_path.exists():\n        jobs.append(run_iqtree_async(msa_path, output_path, log_path, semaphore))\n\nif jobs:\n    _ = await asyncio.gather(*jobs)\n\niq_trees = {}\nfor query in queries_for_iqtree:\n    newick_path = uniclust30_iqtree_dir / f\"{query}.iqtree.newick\"\n    iq_trees[query] = read_newick(newick_path)\n\n\n\n\nVisualizing agreement between tree inference methods\nfrom analysis.plotting import tree_correlation_figures\n\ntree_correlation_figures(trees, iq_trees)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) R² between FastTree and IQ-TREE patristic distances for 10 select MSAs, sorted low to high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Lowest correlation MSA.\n\n\n\n\n\n\n\n\n\n\n\n(c) Median correlation MSA.\n\n\n\n\n\n\n\n\n\n\n\n(d) Highest correlation MSA.\n\n\n\n\n\n\n\nFigure 8: Correlations between patristic distances inferred by FastTree and IQ-TREE. Dashed line is the 1:1 line, solid line is the line of best fit.\n\n\n\n\nInterestingly, Figure 8 shows the agreement between FastTree and IQ-TREE varies significantly. While some families show near-perfect agreement (\\(R^2 &gt; 0.98\\)), others show substantial divergence (\\(R^2 &lt; 0.20\\)), confirming that the choice of inference tool can significantly affect patristic distance to query.\nSince there is significant disagreement between the two methods in some MSAs, is it possible that the sequence weights actually encode more accurate evolutionary distances that are simply obscured by the regression targeting inaccurate distances? If the weights were capturing a refined evolutionary signal that FastTree’s heuristics missed, we would expect our regression model to perform better when tracking a more accurate target.\nTo test this, we compared the difference in explanatory power when modeling the distances of both inference methods. If the sequence weights encode refined information that is obscured by the FastTree approximation, the distances derived from IQ-TREE—which better represent the “true” evolutionary distances—should yield a consistently higher \\(R^2_{\\text{adj}}\\).\n\n\nVisualizing impact of tree inference method on patristic distance prediction\nfrom scipy import stats\n\nfrom analysis.plotting import tree_method_comparison_figure\n\ntree_correlation_r2 = {}\nfasttree_adj_r2 = {}\niqtree_adj_r2 = {}\n\nfor query in iq_trees:\n    iqtree = iq_trees[query]\n    fasttree = trees[query]\n    msa = msas[query]\n    weights = seq_weights[query]\n\n    iqtree_distances = get_patristic_distance(iqtree, query)\n    fasttree_distances = get_patristic_distance(fasttree, query)\n    df = pd.DataFrame({\"iqtree\": iqtree_distances, \"fasttree\": fasttree_distances}).dropna()\n    r, _ = stats.pearsonr(df[\"iqtree\"], df[\"fasttree\"])\n    tree_correlation_r2[query] = r**2\n\n    model_ft, _, _, _, _ = compute_regression_residuals(fasttree, msa, weights, query)\n    fasttree_adj_r2[query] = model_ft.rsquared_adj\n\n    model_iq, _, _, _, _ = compute_regression_residuals(iqtree, msa, weights, query)\n    iqtree_adj_r2[query] = model_iq.rsquared_adj\n\ntree_method_comparison_figure(tree_correlation_r2, fasttree_adj_r2, iqtree_adj_r2)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Difference in adjusted R² (IQ-TREE minus FastTree) for each query, sorted by method agreement.\n\n\n\n\n\n\n\n\n\n\n\n(b) Adjusted R² for FastTree (filled) and IQ-TREE (open) plotted against tree method agreement. Vertical lines connect paired measurements for each query. Colors match Figure 8.\n\n\n\n\n\n\n\nFigure 9: The choice of tree inference method has minimal impact on the sequence weights’ ability to predict patristic distances.\n\n\n\n\nWe find no evidence of such a systematic bias (Figure 9 (a)). The \\(R^2_{\\text{adj}}\\) values fluctuate around zero, indicating that the sequence weights do not track the more rigorous IQ-TREE phylogenies any better than the FastTree approximations.\nInstead, we observe a striking relationship between method agreement and explanatory power (Figure 9 (b)). When the two inference algorithms agree, suggesting a strong phylogenetic signal, the sequence weights can model the evolutionary distances accurately. Conversely, when the methods diverge, the weights fail to capture the distances of either tree, despite IQ-TREE inferring more accurate distances. This suggests the model’s attention mechanism appears to be encountering the same inherent noise of the MSA that prevents dedicated tree-building tools from converging on a single answer.\n\n\nQuantifying MSA Difficulty\nTo verify that these “difficult” MSAs are indeed less informative, we use Pythia 2.0 (Haag and Stamatakis, 2025), a machine-learning-based tool designed to predict the difficulty of phylogenetic reconstruction from MSAs by extracting structural and evolutionary features from the alignments, like site patterns and gap distributions, in order to estimate the statistical confidence of the resulting tree.\n\n\n\n\n\n\nNoteTechnical Note: Reproducing MSA Difficulty Calculations\n\n\n\nDue to strict environment requirements and dependency conflicts (reported issue here), Pythia 2.0 is not compatible with the runtime environment used for this publication. Instead, we performed these calculations in a standalone environment and have uploaded the resulting difficulty scores to a public S3 bucket and to Zenodo. The results are downloaded automatically when the publication is executed. Despite this constraint, we’ve tried to maximize reproducibility by providing full instructions for setting up the Pythia-specific environment and reproducing the MSA difficulty scores here.\n\n\n\n\nCorrelation between MSA difficulty and patristic distance prediction\nfrom arcadia_pycolor.style_defaults import MONOSPACE_FONT\n\nmsa_diff_dir = Path(\"./data/msa_difficulty/\")\nmsa_diff_df = pd.read_csv(msa_diff_dir / \"msa_difficulty.csv\")\nmsa_diff_df[\"difficulty\"] = pd.to_numeric(msa_diff_df[\"difficulty\"], errors=\"coerce\")\nmsa_diff_df = msa_diff_df.dropna()\n\nadjusted_r2_map = dict(zip(results_df[\"Query\"], results_df[\"Adjusted R2\"], strict=False))\nmsa_diff_df[\"adjusted_r2\"] = msa_diff_df[\"query\"].map(adjusted_r2_map)\nmsa_diff_df = msa_diff_df.dropna()\n\nmsa_diff_df_iqtree = msa_diff_df[msa_diff_df[\"query\"].isin(iq_trees)].copy()\nmsa_diff_df_iqtree[\"adjusted_r2_iqtree\"] = msa_diff_df_iqtree[\"query\"].map(iqtree_adj_r2)\nmsa_diff_df_iqtree[\"adjusted_r2_fasttree\"] = msa_diff_df_iqtree[\"query\"].map(fasttree_adj_r2)\nmsa_diff_df_iqtree = msa_diff_df_iqtree.sort_values(\"difficulty\")\n\nqueries_sorted = msa_diff_df_iqtree[\"query\"].tolist()\nx_pos = np.arange(len(queries_sorted))\nwidth = 0.35\n\nlinewidth = 0.0\nfig, ax = plt.subplots()\nax.bar(\n    x_pos - width / 2,\n    msa_diff_df_iqtree[\"adjusted_r2_fasttree\"],\n    width,\n    color=apc.marine.hex_code,\n    edgecolor=\"black\",\n    linewidth=linewidth,\n    label=\"FastTree\",\n)\nax.bar(\n    x_pos + width / 2,\n    msa_diff_df_iqtree[\"adjusted_r2_iqtree\"],\n    width,\n    facecolor=apc.shell.hex_code,\n    edgecolor=\"black\",\n    linewidth=linewidth,\n    label=\"IQ-TREE\",\n)\nax.set_ylabel(r\"Adjusted $R^2$\")\nax.set_xlabel(\"Query (sorted by MSA difficulty)\")\nax.set_xticks(x_pos)\nax.set_xticklabels(queries_sorted, rotation=45, ha=\"right\")\nax.legend()\napc.mpl.style_plot(ax, monospaced_axes=\"y\")\nfig.tight_layout()\nplt.show()\n\nr, p = stats.pearsonr(msa_diff_df[\"difficulty\"], msa_diff_df[\"adjusted_r2\"])\nr2 = r**2\n\nfig, ax = plt.subplots()\nax.scatter(\n    msa_diff_df[\"difficulty\"],\n    msa_diff_df[\"adjusted_r2\"],\n    alpha=0.3,\n    s=15,\n    color=apc.marine.hex_code,\n    edgecolor=\"none\",\n)\nslope, intercept, _, _, _ = stats.linregress(msa_diff_df[\"difficulty\"], msa_diff_df[\"adjusted_r2\"])\nx_fit = np.array([msa_diff_df[\"difficulty\"].min(), msa_diff_df[\"difficulty\"].max()])\nax.plot(x_fit, slope * x_fit + intercept, color=\"#333333\", linewidth=2)\n\nax.set_xlabel(\"MSA difficulty\")\nax.set_ylabel(r\"Adjusted $R^2$\")\nax.text(\n    0.05,\n    0.05,\n    f\"r = {r:.3f}\\n$R^2$ = {r2:.3f}\",\n    transform=ax.transAxes,\n    fontsize=14,\n    fontfamily=MONOSPACE_FONT,\n    verticalalignment=\"bottom\",\n    horizontalalignment=\"left\",\n    bbox=dict(facecolor=\"#FFFFFF\", edgecolor=\"#444444\", linewidth=1.0, alpha=0.8),\n)\napc.mpl.style_plot(ax, monospaced_axes=\"both\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Adjusted R² for FastTree (filled) and IQ-TREE (open) for queries with both tree methods, sorted by MSA difficulty.\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot of MSA difficulty vs adjusted R² (FastTree) across all queries.\n\n\n\n\n\n\nFigure 10: MSA difficulty (as predicted by Pythia) shows weak correlation with patristic distance prediction accuracy.\n\n\n\n\nFigure 10 (a) shows the correspondence between MSA difficulty and agreement between FastTree and IQ-TREE. The number of comparisons is too small to see the full picture, so we also compare MSA difficulty to \\(R_\\text{adj}^2\\) (calculated against FastTree distances) for all MSAs and observe a weak but consistent correlation Figure 10 (b). In conjunction with Figure 9 (b), this confirms our hypothesis that MSA difficulty partially explains how phylogenetically encoded the MSA Pairformer sequence weights are.\nAll in all, these results illustrate that the model is constrained by the same fundamental limits of information theory that govern traditional phylogenetics. However, to truly understand MSA Pairformer’s phylogenetic operating range, it’s important to explore the extent to which MSA Pairformer’s encoding of evolutionary distance via sequence weights is affected by tree topology."
  },
  {
    "objectID": "index.html#the-role-of-tree-shape",
    "href": "index.html#the-role-of-tree-shape",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "The role of tree shape",
    "text": "The role of tree shape\nWe’ve observed that MSA Pairformer’s performance in explaining evolutionary relatedness (Figure 5) and its internal sequence weighting (Figure 7) vary significantly across MSAs. While our analysis of MSA difficulty suggests that a noisy phylogenetic signal can act as a bottleneck, the relatively weak correlation between difficulty and performance Figure 10 indicates that signal recoverability alone cannot account for this variance. We hypothesize that these differences are also driven by the tempo and mode of the evolutionary processes that give rise to the specific phylogenetic tree shape of each protein family.\nTesting this hypothesis requires isolating tree shape from the confounding variable of tree size. As noted previously (Janzen and Etienne, 2024), tree shape statistics are difficult to normalize or compare across trees of different sizes.\nTo control for this, we created a standardized dataset by downsampling all MSAs to a uniform depth of 200 sequences, allowing us to attribute remaining performance differences to topology.\nWe also noticed that some trees have extreme outlier branches, which we pruned prior to subsampling. For more details on this, see the collapsable element below.\n\n\n\n\n\n\nNoteFiltering out trees with outlier branches\n\n\n\n\n\nWe noticed some trees have clades, composed of a small fraction of the total sequences, with a branch length extending far beyond all other tips.\nBelow is such an example.\n\n\nCode\nquery = \"A0A1G9M6E8\"\nmsa = msas[query]\ntree = trees[query].copy()\nweights = seq_weights[query]\ntree_render, fig = plot_tree_and_expected_versus_actual(tree, msa, weights, query)\ndisplay(tree_render)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Phylogenetic tree topology. A distinct clade exhibits branch lengths significantly longer than the rest of the family, likely indicating non-homology or alignment artifacts.\n\n\n\n\n\n\n\n\n\n\n\n(b) Effect on regression analysis. These outliers act as high-leverage points (top right), artificially inflating the \\(R^2\\) metric by dominating the variance, despite poor resolution within the main cluster.\n\n\n\n\n\n\n\nFigure 11: Example of an MSA containing extreme outlier branches.\n\n\n\n\nWhile these outlier branches are distinct, they appear in only a small fraction of the examined MSAs. Because these inputs are sourced from the OpenProteinSet, MSA Pairformer likely encountered these sequences—which we hypothesize are non-homologous—during training. However, we have reason to restrict this analysis to “well-behaved” MSAs, in order to prevent long branch clades from disproportionately biasing the regression fit (Figure 11 (b)). Since goodness of fit serves as our proxy for the model’s ability to ascribe evolutionary relatedness, we exclude these sequences because they act as high-leverage points that artificially inflate the \\(R^2\\); this near-perfect score reflects the vast distance between the outliers and the main cluster, rather than the model’s ability to resolve evolutionary distances within the data of interest.\n\n\n\n\n\nDownsampling trees and MSAs to depth 200\nfrom analysis.sequence import filter_msa_by_tree, write_fasta_like\nfrom analysis.tree import find_outliers_in_tree, write_newick\n\nuniclust30_msa_depth_200_dir = uniclust30_dir / \"msas_depth_200\"\nuniclust30_msa_depth_200_dir.mkdir(parents=True, exist_ok=True)\n\nuniclust30_trees_depth_200_dir = uniclust30_dir / \"trees_depth_200\"\nuniclust30_trees_depth_200_dir.mkdir(parents=True, exist_ok=True)\n\nmsas_depth_200 = {}\ntrees_depth_200 = {}\n\ngap_threshold = 0.2\nmax_clade_fraction = 0.1\n\nfor query in progress(trees.keys()):\n    tree = trees[query]\n    msa = msas[query]\n\n    num_sequences = len(tree.get_leaf_names())\n\n    msa_depth_200_path = uniclust30_msa_depth_200_dir / f\"{query}.a3m\"\n    tree_depth_200_path = uniclust30_trees_depth_200_dir / f\"{query}.fasttree.newick\"\n\n    if not msa_depth_200_path.exists() or not tree_depth_200_path.exists():\n        outlier_ids = find_outliers_in_tree(tree, gap_threshold, max_clade_fraction)\n\n        if num_sequences - len(outlier_ids) &lt; 200:\n            # After removing outliers, the tree has less than our target number of sequences\n            continue\n\n        tree_subset = subset_tree(\n            tree,\n            n=200,\n            force_include=[query],\n            force_exclude=outlier_ids,\n            seed=42,\n        )\n        write_fasta_like(*filter_msa_by_tree(msa, tree_subset), msa_depth_200_path)\n        write_newick(tree_subset, tree_depth_200_path)\n\n    msas_depth_200[query] = MSA(msa_depth_200_path, diverse_select_method=\"none\")\n    trees_depth_200[query] = read_newick(tree_depth_200_path)\n\n\n\n\nCalculating sequence weights\nseq_weights_depth_200_path = uniclust30_dir / \"seq_weights_depth_200.pt\"\n\n# When running on Modal, calculate_sequence_weights serializes MSAs to send to remote GPU workers.\n# Serializing all MSAs at once exceeds Modal's serialization limits, so we batch into groups\n# of 1000. This constraint is specific to Modal's RPC layer. This batching choice is unnecessary\n# yet harmless for local execution.\nseq_weights_depth_200 = {}\nbatch_size = 250\n\n_msa_items = list(msas_depth_200.items())\n\nif seq_weights_depth_200_path.exists():\n    seq_weights_depth_200 = torch.load(seq_weights_depth_200_path, weights_only=True)\nelse:\n    for batch_start in progress(\n        range(0, len(_msa_items), batch_size), desc=\"Calculating sequence weights\"\n    ):\n        _batch_msas = dict(_msa_items[batch_start : batch_start + batch_size])\n        _batch_weights = calculate_sequence_weights(_batch_msas)\n        seq_weights_depth_200.update(_batch_weights)\n\n    torch.save(seq_weights_depth_200, seq_weights_depth_200_path)\n\n\nTo characterize tree shape, we use a combination of global tree metrics, as well as query-centric metrics.\n\n\n\n\n\n\n\n\n\n\n\nCategory\nMetric\nDescription\n\n\n\n\nGlobal\nPhylogenetic diversity\nThe sum of all branch lengths in the tree.\n\n\nGlobal\nColless index\nA measure of tree imbalance, calculated as the sum of the absolute differences in the number of tips descending from the left and right children of each internal node.\n\n\nGlobal\nCherry count\nThe total number of “cherries,” which are pairs of leaves that share an immediate common ancestor.\n\n\nGlobal\nUltrametricity CV\nThe coefficient of variation (standard deviation / mean) of the root-to-tip distances. A value of 0 indicates a perfectly ultrametric tree.\n\n\nQuery-Centric\nPatristic mean\nThe mean of the patristic distances (the sum of branch lengths on the shortest path) from the query sequence to all other leaves.\n\n\nQuery-Centric\nPatristic standard deviation\nThe standard deviation of the patristic distances (the sum of branch lengths on the shortest path) from the query sequence to all other leaves.\n\n\nQuery-Centric\nQuery centrality\nMeasures the query’s position relative to the rest of the tree. Calculated as the ratio of the mean distance from the query to all leaves divided by the mean pairwise distance of all leaves. Values &lt; 1 indicate a central position.\n\n\n\n\n\nTable 1: A summary of global and query-centric metrics used to characterize phylogenetic tree topology.\n\n\n\nFor our global metrics, we adapt recommendations from (Janzen and Etienne, 2024). We also add Ultrametricity CV to quantify how clock-like the tree is (i.e., substitutions accumulate along branches linearly with respect to time), hypothesizing that variation in root-to-tip distances may affect the model’s learning of evolutionary rates. Note, rather than formally rooting phylogenies, trees were midpoint-rooted prior to calculating tree statistics.\nSince global metrics are blind to the query’s position—the anchor for the model’s attention—we also introduce three query-centric metrics. Patristic mean and Patristic standard deviation measures the spread of evolutionary distances from the query, while Query centrality assesses whether the query is topologically central or peripheral. We can now investigate how this full set of features correlates with the model’s explanatory success.\n\n\nCalculating regressions and tree statistics\nimport pandas as pd\nfrom ete3 import Tree\nfrom joblib import Parallel, delayed\nfrom scipy.stats import spearmanr\n\nfrom analysis.regression import regress_and_analyze_features\nfrom analysis.tree import (\n    cherry_count_statistic,\n    colless_statistic,\n    patristic_mean,\n    patristic_std,\n    phylogenetic_diversity_statistic,\n    query_centrality,\n    ultrametricity_cv,\n)\n\n\ndef process_msa(\n    query: str, dist_to_query: np.ndarray, tree: Tree, weights: torch.Tensor\n) -&gt; dict[str, Any]:\n    data = {}\n\n    # Regress the sequence weights against patristic distance.\n    # Perform an ANOVA (type III) to establish explanatory importance\n    # of each layer's sequence weights. Ignore the distance of the\n    # query from itself as an observation.\n    weights = weights[1:, :]\n    dist_to_query = dist_to_query[1:]\n\n    model, anova_table = regress_and_analyze_features(weights, dist_to_query)\n\n    y_pred = model.fittedvalues\n    y_actual = model.model.endog\n\n    data[\"Query\"] = query\n    data[\"R2\"] = model.rsquared\n    data[\"Adjusted R2\"] = model.rsquared_adj\n    data[\"Spearman\"], _ = spearmanr(y_pred, y_actual)\n\n    # Global metrics\n    data[\"Phylogenetic diversity\"] = phylogenetic_diversity_statistic(tree)\n    data[\"Colless\"] = colless_statistic(tree)\n    data[\"Cherry count\"] = cherry_count_statistic(tree)\n    data[\"Ultrametricity CV\"] = ultrametricity_cv(tree)\n\n    # Query-centric metrics\n    data[\"Patristic mean\"] = patristic_mean(tree, query)\n    data[\"Patristic std\"] = patristic_std(tree, query)\n    data[\"Query centrality\"] = query_centrality(tree, query)\n\n    data.update(anova_table[\"percent_sum_sq\"].to_dict())\n\n    return data\n\n\njobs = []\nfor query in seq_weights_depth_200.keys():\n    msa = msas_depth_200[query]\n    tree = trees_depth_200[query]\n\n    weights = seq_weights_depth_200[query]\n    dist_to_query = get_patristic_distance(tree, query)[msa.ids_l].values\n\n    jobs.append(delayed(process_msa)(query, dist_to_query, tree, weights))\n\nresults_depth_200_df = pd.DataFrame(Parallel(-1)(jobs))\n\n\nWe regress the extracted tree features against the model’s performance (\\(R^2_\\text{adj}\\)) and estimate the explained variance of each feature using a Type III ANOVA.\nThe results, visualized in Figure 12 (a), are revealing, with patristic standard deviation and phylogenetic diversity accounting for a combined 53.8% of variance. The directionality of these relationships, shown in Figure 12 (b) and Figure 12 (c), helps us build a theory of the model’s “phylogenetic operating range.”\n\n\nVisualization code\nfrom analysis.plotting import multivariate_regression_figures\n\nmultivariate_regression_figures(results_depth_200_df)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Explained variance from Type III ANOVA of tree topology features regressed against Adjusted R².\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Adjusted \\(R^2\\) decreases with phylogenetic diversity of the tree.\n\n\n\n\n\n\n\n\n\n\n\n(c) Adjusted \\(R^2\\) increases with the standard deviation of patristic distances from the query sequence.\n\n\n\n\n\n\n\nFigure 12: Tree topology features explain variation in MSA Pairformer’s ability to predict patristic distance. Phylogenetic diversity and patristic standard deviation together account for the majority of explained variance.\n\n\n\n\nFirst, we observe a negative correlation between performance and phylogenetic diversity. As the sum of branch lengths in the tree increases, the model’s ability to map sequence weights to evolutionary distance degrades. This relationship likely reflects the fact that trees with high total phylogenetic diversity may feature many long terminal branches (i.e., with many derived substitutions) and little clade-like structure that obscure the evolutionary relationships, making the mapping from sequence weights to distance significantly more ambiguous. It is worth noting that these scenarios are also the most challenging for tree inference methods due to the scarcity of consistent signal between clades (e.g., long-branch-attraction (Felsenstein, 1978)), implying that the observed performance degradation may be partially caused by reduced accuracy of our inferred trees.\nConversely, we see a strong positive correlation with patristic standard deviation. This metric quantifies the variance in evolutionary distance relative to the query. A high standard deviation implies the presence of a strong gradient where there exists a mixture of close and distant homologs. A low standard deviation implies a “star-like” topology where most sequences are roughly equidistant from the query, or a highly “balanced” tree topology characterized by a largely uniform branching pattern.\nThis finding suggests that the query-biased attention mechanism may serve primarily as a contrast detector. The model excels when it is presented with a clear evolutionary gradient to sort (i.e., proteins can be sorted into few deeply-diverging clades). When that gradient is flattened (i.e., many shallow-diverging or closely related clades), as is the case in MSAs with low patristic standard deviation, the model lacks the necessary contrast to effectively stratify sequences by evolutionary distance.\nIn summary, the efficacy with which MSA Pairformer encodes evolutionary relatedness in its sequence weights isn’t uniformly distributed across protein families and depends heavily on the sequences in the input MSA and their underlying evolutionary processes. The mechanism seems most effective in families that offer a clear gradient of relationships to the query, and its resolution diminishes when faced with lots of diversity or a lack of relative evolutionary contrast."
  },
  {
    "objectID": "index.html#sequence-weight-usage",
    "href": "index.html#sequence-weight-usage",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Sequence weight usage",
    "text": "Sequence weight usage\nWe’ve demonstrated that the extent to which query-biased sequence weights reflect evolutionary distance varies significantly across MSAs (Figure 5). Akiyama et al. (2025) propose that these weights function by biasing the model’s internal representations toward “evolutionarily relevant” sequences. Based on this, we hypothesize that MSAs where the weights strongly correlate with evolutionary distance will be the most sensitive to ablation. Specifically, for these well-modeled MSAs, replacing the learned weights with a uniform average should result in the largest performance degradation, as the high-fidelity evolutionary signal is lost to phylogenetic averaging.\nTo test our hypothesis, we predict residue-residue contacts for the ~5,000 MSAs in our dataset, each subsampled to a depth of 200. We measure prediction accuracy with \\(P@L\\)8 under two separate conditions. Under the first condition, we predict contacts using the model’s native learned sequence weights (\\(P@L\\)). In the second, we substitute these with uniform weights, treating all sequences equally, and denote the resulting accuracy as \\(P@L^{(\\text{uni})}\\). We measure the gain in performance when using sequence weights with \\(\\Delta P@L = P@L - P@L^{(\\text{uni})}\\).\n8 \\(P@L\\) is a metric for contact prediction accuracy, defined as the precision of the top \\(L\\) predicted beta-carbon contacts, where \\(L\\) is the protein sequence length. Predictions are ranked by confidence score, and only “long-range” contacts (residues separated by \\(\\ge 24\\) positions in the primary sequence) are evaluated.\n\nCalculating P@L with/without sequence weights\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\n\nfrom analysis.open_protein_set import fetch_pdbs\nfrom analysis.pairformer import calculate_cb_contacts\nfrom analysis.regression import regress_and_analyze_features\nfrom analysis.structure import (\n    calculate_long_range_p_at_l,\n    load_structure,\n    split_structure_by_atom_type,\n)\n\npdb_dir = uniclust30_dir / \"pdbs\"\npdb_paths = fetch_pdbs(msas_depth_200.keys(), pdb_dir)\n\nangstrom_cutoff = 8.0\n\n\ndef euclidean_distance_tensor(coords: torch.Tensor) -&gt; torch.Tensor:\n    return torch.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=-1)\n\n\ncb_contacts_pdb = {}\nfor query, pdb_path in progress(pdb_paths.items(), desc=\"Calculating CB contacts\"):\n    _, cb_coords, _, _ = split_structure_by_atom_type(load_structure(pdb_path))\n    cb_dist = euclidean_distance_tensor(cb_coords)\n    cb_contacts_pdb[query] = cb_dist &lt; angstrom_cutoff\n\n\ndef load_or_compute(msa_dict, save_path, compute_fn, batch_size=250, desc=\"Processing\"):\n    \"\"\"\n    Generic utility to load cached results or compute them in batches using a specific function.\n\n    Args:\n        msa_dict: Dictionary of MSAs to process.\n        save_path: Path object where results are saved/loaded.\n        compute_fn: Function to call on each batch (e.g., calculate_sequence_weights).\n        batch_size: Number of items to process per batch (to avoid Modal RPC limits).\n        desc: Description for the progress bar.\n    \"\"\"\n    if save_path.exists():\n        return torch.load(save_path, weights_only=True)\n\n    results = {}\n    msa_items = list(msa_dict.items())\n\n    for batch_start in range(0, len(msa_items), batch_size):\n        batch_msas = dict(msa_items[batch_start : batch_start + batch_size])\n        # dynamically call the function passed in\n        batch_results = compute_fn(batch_msas)\n        results.update(batch_results)\n\n    torch.save(results, save_path)\n    return results\n\n\ncalculate_cb_contacts_uniform = partial(calculate_cb_contacts, query_biasing=False)\n\ncb_contacts = load_or_compute(\n    msa_dict=msas_depth_200,\n    save_path=uniclust30_dir / \"cb_contacts.pt\",\n    compute_fn=calculate_cb_contacts,\n    desc=\"Predicting CB contacts\",\n)\n\ncb_contacts_uniform = load_or_compute(\n    msa_dict=msas_depth_200,\n    save_path=uniclust30_dir / \"cb_contacts_uniform.pt\",\n    compute_fn=calculate_cb_contacts_uniform,\n    desc=\"Predicting CB contacts (uniform)\",\n)\n\ndata_rows = []\nqueries = list(cb_contacts.keys())\n\nfor query in queries:\n    pred = cb_contacts[query].squeeze()\n    pred_uniform = cb_contacts_uniform[query].squeeze()\n\n    ground_truth = cb_contacts_pdb[query]\n\n    # Skip if residue counts mismatch (Missing PDB residues)\n    if pred.size(1) != ground_truth.size(1):\n        continue\n\n    p_at_l = calculate_long_range_p_at_l(pred, ground_truth)\n    p_at_l_uniform = calculate_long_range_p_at_l(pred_uniform, ground_truth)\n\n    data_rows.append(\n        {\n            \"Query\": query,\n            \"P@L\": p_at_l,\n            \"P@L_uniform\": p_at_l_uniform,\n            \"P@L_delta\": p_at_l - p_at_l_uniform,\n        }\n    )\n\ncontact_results = pd.DataFrame(data_rows)\ncontact_results\n\ncols_to_drop = [\"P@L\", \"P@L_uniform\", \"P@L_delta\"]\nfor col in cols_to_drop:\n    if col in results_depth_200_df.columns:\n        results_depth_200_df.drop(col, axis=1, inplace=True)\n\nresults_depth_200_df = pd.merge(results_depth_200_df, contact_results, how=\"left\", on=\"Query\")\nresults_depth_200_df\n\n\n\n\nVisualizing the results\nfrom matplotlib.colors import TwoSlopeNorm\n\n\ndef patl_delta_plot(data, bins=100, lims=None):\n    plt.close()\n\n    if lims is None:\n        lims = data.min(), data.max()\n\n    fig, ax = plt.subplots()\n\n    counts, bin_edges = np.histogram(data, bins=bins)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    gradient = apc.Gradient(\n        name=\"amber_seaweed\",\n        colors=[apc.amber, apc.amber, apc.oat, apc.seaweed, apc.seaweed],\n        values=[0, 0.2, 0.5, 0.8, 1],\n    )\n\n    cmap = gradient.to_mpl_cmap()\n    norm = TwoSlopeNorm(vmin=lims[0], vcenter=0, vmax=lims[1])\n    colors = [cmap(norm(c)) for c in bin_centers]\n\n    ax.bar(bin_centers, counts, width=np.diff(bin_edges), linewidth=0.0, color=colors)\n\n    ax.set_xlabel(r\"$\\Delta$ P@L\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xlim(lims)\n    ax.axvline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n    ax.axvline(data.mean(), color=\"k\", linestyle=\"-\", alpha=0.7)\n\n    apc.mpl.style_plot(ax, monospaced_axes=\"both\")\n\n    return fig, cmap, norm\n\n\nfig, cmap, norm = patl_delta_plot(\n    results_depth_200_df[\"P@L_delta\"].dropna(), bins=150, lims=(-0.05, 0.05)\n)\nplt.show()\n\nfig, ax = plt.subplots()\nscatter = ax.scatter(\n    results_depth_200_df[\"Adjusted R2\"],\n    results_depth_200_df[\"P@L_delta\"],\n    c=results_depth_200_df[\"P@L_delta\"],\n    cmap=cmap,\n    norm=norm,\n    alpha=0.6,\n)\nax.set_ylabel(r\"$\\Delta$ P@L\")\nax.set_xlabel(\"Adjusted $R^2$\")\napc.mpl.style_plot(ax, monospaced_axes=\"both\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram of \\(\\Delta P@L\\). Positive values indicate predictions that benefit from sequence weighting and negative values indicate predictions that are hurt by sequence weighting. The dashed line marks zero and the solid line shows the mean. The MSA dataset used matches that in Figure 12.\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot between \\(R^2_{\\text{adj}}\\) and \\(\\Delta P@L\\). MSAs where tree topology better explains sequence weights (higher \\(R^2_{\\text{adj}}\\)) do not show a clear trend toward benefiting more from sequence weighting.\n\n\n\n\n\n\n\nFigure 13: Effect of sequence weighting on contact prediction accuracy. Points are colored by \\(\\Delta P@L\\), with amber indicating predictions hurt by weighting and green indicating predictions that benefit.\n\n\n\n\n\n\n\n\n\n\nTipIs the same trend observed for the CASP15 dataset?\n\n\n\n\n\nA concern arose regarding data leakage: since our targets came from the OpenProteinSet, the model’s contact head had likely seen these queries and structures during training. To rule out this confounder, we validated our findings by repeating the analysis on CASP15, the standard held-out dataset used in Akiyama et al. (2025).\nThe results in Figure 14 mirror those in Figure 13 (a): while weighting slightly outperforms averaging overall, averaging outperforms weighting in nearly half the individual examples.\n\n\nRepeating the analysis on CASP15\ncasp15_dir = Path(\"data\") / \"casp15\"\n\ncasp15_msas = {}\nfor path in (casp15_dir / \"msas\").glob(\"*.a3m\"):\n    msa = MSA(path, max_seqs=512, max_length=1024)\n    casp15_msas[path.stem] = msa\n\ncalculate_cb_contacts_uniform = partial(calculate_cb_contacts, query_biasing=False)\n\ncasp15_cb_contacts = load_or_compute(\n    msa_dict=casp15_msas,\n    save_path=casp15_dir / \"casp15_cb_contacts.pt\",\n    compute_fn=calculate_cb_contacts,\n    desc=\"Predicting CB contacts\",\n)\n\ncasp15_cb_contacts_uniform = load_or_compute(\n    msa_dict=casp15_msas,\n    save_path=casp15_dir / \"casp15_cb_contacts_uniform.pt\",\n    compute_fn=calculate_cb_contacts_uniform,\n    desc=\"Predicting CB contacts (uniform)\",\n)\n\ncasp15_pdb_paths = {path.stem: path for path in (casp15_dir / \"targets\").glob(\"*.pdb\")}\n\ncasp15_cb_contacts_truth = {}\nfor query in casp15_cb_contacts:\n    structure = load_structure(casp15_pdb_paths[query])\n    _, cb_coords, _, _ = split_structure_by_atom_type(structure)\n    cb_dist = euclidean_distance_tensor(cb_coords)\n    casp15_cb_contacts_truth[query] = cb_dist &lt; angstrom_cutoff\n\n    data_rows = []\n\nqueries = list(casp15_cb_contacts.keys())\n\nfor query in queries:\n    pred = casp15_cb_contacts[query].squeeze()\n    pred_uniform = casp15_cb_contacts_uniform[query].squeeze()\n    ground_truth = casp15_cb_contacts_truth[query]\n    p_at_l = calculate_long_range_p_at_l(pred, ground_truth)\n    p_at_l_uniform = calculate_long_range_p_at_l(pred_uniform, ground_truth)\n\n    data_rows.append(\n        {\n            \"Query\": query,\n            \"P@L\": p_at_l,\n            \"P@L_uniform\": p_at_l_uniform,\n            \"P@L_delta\": p_at_l - p_at_l_uniform,\n            \"domain_length\": pred.size(0),\n        }\n    )\n\ncasp15_contact_results = pd.DataFrame(data_rows)\nfig = patl_delta_plot(casp15_contact_results[\"P@L_delta\"], bins=20)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14: Histogram of \\(\\Delta P@L\\) for the CASP15 dataset. Dashed line marks zero and the solid line shows the mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionDiscrepancy with the original paper\n\n\n\n\n\nWe note that the original paper states, “We […] find that using query-biased attention increases MSA Pairformer’s contact precision from 0.50 to 0.52”, when discussing their contact prediction results from the CASP15 dataset (Akiyama et al., 2025). These results differ from ours:\n\n\nCode\npatl_with = casp15_contact_results[\"P@L\"].mean()\npatl_without = casp15_contact_results[\"P@L_uniform\"].mean()\nprint(f\"Long-range P@L with sequence weights: {patl_with:.3f}\")\nprint(f\"Long-range P@L with uniform weights: {patl_without:.3f}\")\n\n\nLong-range P@L with sequence weights: 0.553\nLong-range P@L with uniform weights: 0.552\n\n\nWe report higher \\(P@L\\) scores in both cases compared to the original study. This may be due to differences in our methodology for MSA generation. While the authors queried UniRef30 with HHblits, we use a ColabFold MSA server, which queries a collation of databases (including UniRef30) with MMSeqs2.\nIn contrast to the original study, we observe no significant difference in performance with/without sequence weighting. We find it plausible that their uniform baseline (0.50) was evaluated at the end of pre-training, before the model was fine-tuned with query-biased attention. If true, this would differ fundamentally from our approach, where we use the same fine-tuned architecture for both passes, simply injecting uniform weights at inference time to isolate the mechanism’s effect.\nWe invite the authors to comment on the discrepancy, as it has important implications for the model’s proposed mechanism. All our code is available with the hopes that any errors in our analysis can be identified and corrected.\n\n\n\nOur original intent was to establish a direct causal link between the evolutionary information in the weights and the magnitude of downstream improvement. Instead, the data reveals that sequence weighting at inference time provides, at best, modest gains in contact prediction performance. We conclude that the active application of the query-biased outer product doesn’t appear to be a load-bearing pillar of the architecture for contact prediction, but rather a subtle optimization. And while we observe a measurable improvement in aggregate (~0.008 increase in precision), the results are stochastic, often yielding significantly worse results than simple, phylogenetic averaging.\nFinally, we propose the possibility that the query-biased outer product may play an important effect in guiding the trajectory of the weights during training, while being less relevant during inference."
  },
  {
    "objectID": "index.html#sequence-weight-information-flow",
    "href": "index.html#sequence-weight-information-flow",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Sequence weight information flow",
    "text": "Sequence weight information flow\nWe have established that the aggregate effect of sequence weighting on contact prediction is small. However, this average disguises a significant split in behavior: for some MSAs, sequence weighting provides a massive boost, while for others, it causes a significant degradation.\nTo understand the mechanics of this divergence, we isolate the 25 MSAs that benefited most from sequence weighting (the positive cohort) and the 25 MSAs that were penalized most (the negative cohort).\n\n\nPicking out the top 25 most benefited/penalized examples\nk = 25\n_negative = results_depth_200_df.nsmallest(k, \"P@L_delta\")\n_negative[\"kind\"] = \"negative\"\n_positive = results_depth_200_df.nlargest(k, \"P@L_delta\")\n_positive[\"kind\"] = \"positive\"\nextreme_cases = pd.concat([_negative, _positive], axis=0)\n\n\nTo test this, we employ a shuffling procedure that reassigns the weight of sequence \\(A\\) to sequence \\(B\\), thereby preserving the global statistical distribution of the weights while destroying their specific phylogenetic mapping. We implement this using two distinct strategies. The first, which we call cumulative shuffling, acts as a progressive reset where we shuffle the weights for all layers \\(L\\) greater than or equal to a start layer \\(N\\). We hypothesize that shuffling from layer 0 will mimic the uniform baseline, and that as we advance the start layer \\(N\\) deeper into the network, we effectively allow more of the “true” signal to pass through, gradually recovering the original performance. The second strategy, single layer shuffling, acts as a targeted intervention where we shuffle the weights only at a specific layer \\(N\\), allowing us to identify which specific layers are most influential in driving this optimization.\n\n\nPredicting contacts with shuffled weights\nreplicates = 4\nmsas_top_n = {}\nfor query in extreme_cases[\"Query\"]:\n    msa = msas_depth_200[query]\n    for replicate in range(replicates):\n        key = f\"{query}_{replicate:02d}\"\n        msas_top_n[key] = msa\n\nlayer_shuffling_dir = uniclust30_dir / \"layer_shuffling\"\nlayer_shuffling_dir.mkdir(exist_ok=True)\n\nsingle_layer_shuffling_dir = layer_shuffling_dir / \"single\"\nsingle_layer_shuffling_dir.mkdir(exist_ok=True)\n\ncumulative_layer_shuffling_dir = layer_shuffling_dir / \"cumulative\"\ncumulative_layer_shuffling_dir.mkdir(exist_ok=True)\n\nfor layer in range(22):\n    calculate_cb_contacts_shuffled_cumulative = partial(\n        calculate_cb_contacts,\n        query_biasing=True,\n        shuffled=True,\n        shuffled_layers=list(range(layer, 22)),\n    )\n\n    load_or_compute(\n        msa_dict=msas_top_n,\n        save_path=cumulative_layer_shuffling_dir / f\"cb_contacts_layer_{layer:02d}.pt\",\n        compute_fn=calculate_cb_contacts_shuffled_cumulative,\n        desc=f\"Predicting CB contacts (layer {layer}, cumulative)\",\n    )\n\n    calculate_cb_contacts_shuffled_single = partial(\n        calculate_cb_contacts,\n        query_biasing=True,\n        shuffled=True,\n        shuffled_layers=[layer],\n    )\n\n    load_or_compute(\n        msa_dict=msas_top_n,\n        save_path=single_layer_shuffling_dir / f\"cb_contacts_layer_{layer:02d}.pt\",\n        compute_fn=calculate_cb_contacts_shuffled_single,\n        desc=f\"Predicting CB contacts (layer {layer}, single)\",\n    )\n\n\nWe now compile the results. For every shuffled run, we calculate the deviation from the true performance (i.e., the performance using the correct sequence weights).\n\n\nCompiling P@L scores for shuffled/unshuffled predictions\nshuffle_data = []\n\nfor layer in range(22):\n    prediction_path = single_layer_shuffling_dir / f\"cb_contacts_layer_{layer:02d}.pt\"\n    contact_preds = torch.load(prediction_path, weights_only=True)\n\n    for key, pred in contact_preds.items():\n        query = key.split(\"_\")[0]\n        rep = int(key.split(\"_\")[1])\n        ground_truth = cb_contacts_pdb[query]\n        p_at_l = calculate_long_range_p_at_l(pred.squeeze(), ground_truth)\n        true_p_at_l = contact_results.query(\"Query == @query\").iloc[0][\"P@L\"]\n        uniform_p_at_l = contact_results.query(\"Query == @query\").iloc[0][\"P@L_uniform\"]\n        kind = extreme_cases.query(\"Query == @query\").iloc[0][\"kind\"]\n\n        entry = {\n            \"Query\": query,\n            \"Cumulative\": False,\n            \"Kind\": kind,\n            \"Shuffled layer\": layer,\n            \"Replicate\": rep,\n            \"P@L\": true_p_at_l,\n            \"P@L_uniform\": uniform_p_at_l,\n            \"P@L_shuffled\": p_at_l,\n            \"P@L_delta\": p_at_l - true_p_at_l,\n        }\n\n        shuffle_data.append(entry)\n\n    prediction_path = cumulative_layer_shuffling_dir / f\"cb_contacts_layer_{layer:02d}.pt\"\n    contact_preds = torch.load(prediction_path, weights_only=True)\n\n    for key, pred in contact_preds.items():\n        query = key.split(\"_\")[0]\n        rep = int(key.split(\"_\")[1])\n        ground_truth = cb_contacts_pdb[query]\n        p_at_l = calculate_long_range_p_at_l(pred.squeeze(), ground_truth)\n        true_p_at_l = contact_results.query(\"Query == @query\").iloc[0][\"P@L\"]\n        uniform_p_at_l = contact_results.query(\"Query == @query\").iloc[0][\"P@L_uniform\"]\n        kind = extreme_cases.query(\"Query == @query\").iloc[0][\"kind\"]\n\n        entry = {\n            \"Query\": query,\n            \"Cumulative\": True,\n            \"Kind\": kind,\n            \"Shuffled layer\": layer,\n            \"Replicate\": rep,\n            \"P@L\": true_p_at_l,\n            \"P@L_uniform\": uniform_p_at_l,\n            \"P@L_shuffled\": p_at_l,\n            \"P@L_delta\": p_at_l - true_p_at_l,\n        }\n\n        shuffle_data.append(entry)\n\nshuffle_df = pd.DataFrame(shuffle_data)\nshuffle_df\n\n\n\n\n\n  \n    \n      \n      Query\n      Cumulative\n      Kind\n      Shuffled layer\n      Replicate\n      P@L\n      P@L_uniform\n      P@L_shuffled\n      P@L_delta\n    \n  \n  \n    \n      0\n      A0A1G9D7T6\n      False\n      positive\n      0\n      0\n      0.713080\n      0.354430\n      0.708861\n      -0.004219\n    \n    \n      1\n      A0A1G9D7T6\n      False\n      positive\n      0\n      1\n      0.713080\n      0.354430\n      0.708861\n      -0.004219\n    \n    \n      2\n      A0A1G9D7T6\n      False\n      positive\n      0\n      2\n      0.713080\n      0.354430\n      0.708861\n      -0.004219\n    \n    \n      3\n      A0A1G9D7T6\n      False\n      positive\n      0\n      3\n      0.713080\n      0.354430\n      0.713080\n      0.000000\n    \n    \n      4\n      F4CP98\n      False\n      positive\n      0\n      0\n      0.792717\n      0.537815\n      0.792717\n      0.000000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      8795\n      A0A0S7LXA2\n      True\n      negative\n      21\n      3\n      0.476744\n      0.515504\n      0.476744\n      0.000000\n    \n    \n      8796\n      A0A097P7J4\n      True\n      negative\n      21\n      0\n      0.722488\n      0.760766\n      0.722488\n      0.000000\n    \n    \n      8797\n      A0A097P7J4\n      True\n      negative\n      21\n      1\n      0.722488\n      0.760766\n      0.722488\n      0.000000\n    \n    \n      8798\n      A0A097P7J4\n      True\n      negative\n      21\n      2\n      0.722488\n      0.760766\n      0.722488\n      0.000000\n    \n    \n      8799\n      A0A097P7J4\n      True\n      negative\n      21\n      3\n      0.722488\n      0.760766\n      0.722488\n      0.000000\n    \n  \n\n8800 rows × 9 columns\n\n\n\n\n\nVisualizing the results\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate normalization metrics\n# max_delta represents the total impact of sequence weighting (True - Uniform)\nshuffle_df[\"max_delta\"] = shuffle_df[\"P@L\"] - shuffle_df[\"P@L_uniform\"]\nshuffle_df[\"P@L_delta_normalized\"] = shuffle_df[\"P@L_delta\"] / shuffle_df[\"max_delta\"].abs()\n\n# Filter for cumulative shuffling data\ncumulative_data = shuffle_df[shuffle_df[\"Cumulative\"] == True].copy()  # noqa: E712\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot distributions\nhue_order = [\"negative\", \"positive\"]\nsns.boxplot(\n    data=cumulative_data,\n    x=\"Shuffled layer\",\n    y=\"P@L_delta_normalized\",\n    hue=\"Kind\",\n    hue_order=hue_order,\n    palette={\"positive\": \"white\", \"negative\": \"white\"},\n    ax=ax,\n    showfliers=False,\n    showcaps=False,\n    boxprops={\"linewidth\": 0},\n    whiskerprops={\"linewidth\": 2},\n    medianprops={\"linewidth\": 1},\n)\n\n# Color patches based on their actual y-position in data coordinates\nfrom matplotlib.colors import to_rgba\n\ninv_transform = ax.transData.inverted()\npatch_colors = []\nfor patch in ax.patches:\n    bbox = patch.get_extents()\n    # Convert pixel coords to data coords\n    y0_data = inv_transform.transform((bbox.x0, bbox.y0))[1]\n    y1_data = inv_transform.transform((bbox.x1, bbox.y1))[1]\n    y_center = (y0_data + y1_data) / 2\n    color = apc.seaweed.hex_code if y_center &lt; 0 else apc.amber.hex_code\n    patch.set_edgecolor(color)\n    patch.set_facecolor(to_rgba(color, 0.7))\n    patch_colors.append(color)\n\n# Lines: 3 per box (2 whiskers + 1 median), same ordering as patches\nn_boxes = len(ax.patches)\nn_lines_per_box = 3\nfor i, line in enumerate(ax.lines[: n_boxes * n_lines_per_box]):\n    box_idx = i // n_lines_per_box\n    line.set_color(patch_colors[box_idx])\n\n# Styling\nax.axhline(0, color=apc.slate, linestyle=\"--\", linewidth=1, alpha=0.5)\nax.set_xlabel(\"Start layer of shuffling\")\nax.set_ylabel(\"Normalized P@L Delta\")\n\n# Ensure every layer number is visible\nax.set_xticks(range(23))\nax.set_xlim(-0.5, 22.5)\n\n# Manual legend\nfrom matplotlib.patches import Patch\n\nlegend_elements = [\n    Patch(\n        facecolor=to_rgba(apc.seaweed.hex_code, 0.6),\n        edgecolor=apc.seaweed.hex_code,\n        linewidth=1.5,\n        label=\"Positive cohort\",\n    ),\n    Patch(\n        facecolor=to_rgba(apc.amber.hex_code, 0.6),\n        edgecolor=apc.amber.hex_code,\n        linewidth=1.5,\n        label=\"Negative cohort\",\n    ),\n]\nax.legend(handles=legend_elements, frameon=False, loc=\"lower right\", bbox_to_anchor=(1.0, 0.1))\n\napc.mpl.style_plot(ax, monospaced_axes=\"both\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 15: Cumulative shuffling results (Normalized). The distribution of normalized performance deltas is shown for each start layer. The x-axis indicates the start layer \\(N\\) for cumulative shuffling (shuffling layers \\(N\\) through 21). At \\(N=0\\), all weights are shuffled. At \\(N=22\\), no weights are shuffled. The y-axis represents the normalized deviation from the true performance (\\(P@L_{\\text{delta}} / |P@L - P@L_{\\text{uniform}}|\\)).\n\n\n\n\n\nIn Figure 15, the x-axis represents the layer at which we stop shuffling. For the positive cohort, as we stop shuffling early layers and allow the model’s true weights to persist (moving right on the x-axis), performance recovers. This confirms the sequence weights are providing a positive signal that accumulates through the network.\nThe negative cohort tells an inverse story. Shuffling the weights at early layers actually improves performance relative to the true model execution. By destroying the learned sequence weights in the early layers, we force the model back to the uniform baseline, effectively saving it from its own bad intuition. As we stop shuffling (moving right), the harmful signal re-enters the stream, and performance degrades. This implies that for the negative cohort, the model attributes sequence weights that mislead the model.\nTo determine which layers contribute to the success of the positive cohort and the demise of the negative cohort, we look at the single layer shuffling results.\n\n\nVisualizing the results\nfrom matplotlib.colors import to_rgba\n\n# Filter for single layer shuffling data\nsingle_data = shuffle_df[shuffle_df[\"Cumulative\"] == False].copy()  # noqa: E712\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.barplot(\n    data=single_data,\n    x=\"Shuffled layer\",\n    y=\"P@L_delta_normalized\",\n    hue=\"Kind\",\n    hue_order=[\"positive\", \"negative\"],\n    palette={\n        \"positive\": apc.seaweed.hex_code,\n        \"negative\": apc.amber.hex_code,\n    },\n    ax=ax,\n    dodge=False,\n    errorbar=(\"ci\", 95),\n    err_kws={\"linewidth\": 2},\n)\n\n# Make bar fills translucent\nfor patch in ax.patches:\n    patch.set_alpha(0.7)\n\n# Color error bars to match their bars (full opacity)\nlines_per_err = 1  # no capsize\nfor i, line in enumerate(ax.get_lines()):\n    rgba = ax.patches[i // lines_per_err].get_facecolor()\n    line.set_color(rgba[:3])  # RGB only, full opacity\n\nax.axhline(0, color=apc.slate, linestyle=\"--\", linewidth=1, alpha=0.5)\nax.set_xlabel(\"Shuffled layer\")\nax.set_ylabel(\"Normalized P@L Delta\")\n\nax.set_xticks(range(22))\nax.set_xlim(-0.5, 21.5)\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(\n    handles=handles,\n    labels=[\"Positive cohort\", \"Negative cohort\"],\n    frameon=False,\n    loc=\"lower right\",\n    bbox_to_anchor=(1.0, 0.1),\n)\n\napc.mpl.style_plot(ax, monospaced_axes=\"both\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 16: Single layer shuffling results (normalized). The y-axis shows the normalized impact of shuffling a single layer. Bar charts show the mean impact across replicates and queries, with error bars representing the 95% confidence interval. Bars are aligned to show the divergence between the cohorts. The critical sensitivity of layers 5-11 is visible as the region with the largest deviations from zero.\n\n\n\n\n\nFigure 16 highlights that the sequence weight mechanism is most sensitive in the middle layers. Crucially, these are the same layers for both groups. The layers that boost performance in the positive cohort are also the layers that hurt performance in the negative cohort. This indicates that the mechanism for incorporating sequence weights is structurally consistent across the layers, but blindly integrates the calculated signal regardless of whether it helps or hurts prediction."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "MSA-based pLMs encode evolutionary distance but don’t reliably exploit it –",
    "section": "Conclusion",
    "text": "Conclusion\nMSA Pairformer’s reported performance in contact prediction and variant fitness prediction represents an emerging trend in protein language modeling: a move away from single-sequence models with massive parameter counts and towards architectures that leverage evolutionary context at inference in the form of MSAs. In this work, we investigated several key questions that get at the crux of what the future may hold for MSA-based language models.\nWe started with a simple question: Does MSA Pairformer learn phylogenetic relationships? By correlating sequence weights with patristic distances across thousands of diverse families, we confirmed that the model effectively learns to upweight evolutionarily close sequences and downweight distant ones. Some layers (e.g., layer 11) act as strong phylogenetic distance filters in the model’s row-wise attention mechanism.\nWe then illustrated that MSA Pairformer suffers from the same fundamental information constraints as classical phylogenetic methods. By evaluating MSA difficulty and comparing method agreement between FastTree and IQ-TREE, we found that the model’s ability to resolve evolutionary distance is capped by the intrinsic signal quality of the MSA. In difficult MSAs characterized by high noise or limited contrast, the model’s sequence weights failed to track evolutionary distance, implying they’re susceptible to the same signal-to-noise bottlenecks faced in formal phylogenetics.\nHowever, our analysis reveals a critical distinction between encoding phylogenetic relationships and leveraging them. While the model successfully encodes phylogeny in its sequence weights, despite not being trained on the task, this signal does not consistently translate to improved downstream performance. In our contact prediction benchmarks, the native query-biased weights offered only a very marginal improvement over a uniform baseline, and in nearly half the cases, simple phylogenetic averaging actually produced better results. We found that specific layers drive the model’s deviation from the uniform baseline. However, these layers lack a trust mechanism in that they apply weights with a fixed intensity regardless of whether the resulting signal improves or degrades the final prediction accuracy.\nAs the shift toward MSA-based models continues, our results highlight the critical importance of MSA construction. With the onus now on users to provide evolutionary context at inference time, the specific selection of sequences and the overall quality of the MSA will directly impact accuracy. Similarly, for model developers, the quality of MSAs used for training will dictate the extent to which a model can effectively encode and utilize evolutionary information. The decoupling we observed between signal encoding and structural utility suggests that future architectures might benefit from being more selective. Success may depend on a model’s ability to gauge the reliability of an input MSA’s phylogenetic signal before allowing those weights to modulate the structural output."
  },
  {
    "objectID": "pages/SETUP.html",
    "href": "pages/SETUP.html",
    "title": "Setup",
    "section": "",
    "text": "This document details how to create a local copy of this pub’s codebase, setup your compute environment, and reproduce the pub itself. This will enable you to experiment with the analysis in the pub and, optionally, contribute revisions to it.\n\n\nThe codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/2025-phylogenetic-analysis-of-msa-pairformer.git --recurse-submodules\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n 2025-phylogenetic-analysis-of-msa-pairformer --file env.yml\nconda activate 2025-phylogenetic-analysis-of-msa-pairformer\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#obtain-local-copy",
    "href": "pages/SETUP.html#obtain-local-copy",
    "title": "Setup",
    "section": "",
    "text": "The codebase is hosted on GitHub and can be found here.\nTo obtain a local copy of this repo, you can either clone it directly or fork it to your own GitHub account, then clone your fork. If you aren’t sure what’s best, our suggestion is to clone directly unless you both (1) want to propose a revision for the publication and (2) are not an employee of Arcadia Science.\nTo clone:\ngit clone https://github.com/Arcadia-Science/2025-phylogenetic-analysis-of-msa-pairformer.git --recurse-submodules"
  },
  {
    "objectID": "pages/SETUP.html#installation",
    "href": "pages/SETUP.html#installation",
    "title": "Setup",
    "section": "",
    "text": "Important\n\n\n\nThe publication is rendered with Quarto. If you don’t have it installed (check with quarto --version), you can install it here.\n\n\nThis repository uses conda to manage the computational and build environment. If you don’t have it installed (check with conda --version), you can find operating system-specific instructions for installing miniconda here. After installing, run the following commands to create and activate the environment.\nconda env create -n 2025-phylogenetic-analysis-of-msa-pairformer --file env.yml\nconda activate 2025-phylogenetic-analysis-of-msa-pairformer\nNow, install any internal packages in the repository:\npip install -e .\nAnd finally, if you plan to submit a pull request, install the pre-commit hooks:\npre-commit install"
  },
  {
    "objectID": "pages/SETUP.html#reproduce",
    "href": "pages/SETUP.html#reproduce",
    "title": "Setup",
    "section": "Reproduce",
    "text": "Reproduce\nThe best way to ensure you’ve correctly set up your code and compute environment is to reproduce this work. Fortunately, the analysis, and therefore the publication itself, can be reproduced with the following command:\nmake execute\n(Make sure you’re in the conda environment you created above)\nThis will execute and render the notebook index.ipynb, then build the publication site. To preview the site, use\nmake preview\nThis will open a local instance of the publication in your default browser."
  },
  {
    "objectID": "pages/SETUP.html#modify",
    "href": "pages/SETUP.html#modify",
    "title": "Setup",
    "section": "Modify",
    "text": "Modify\nTo modify or extend any analyses, open up index.ipynb with Jupyter or your favorite IDE. To preview changes as you modify the notebook, run make preview again and leave the command running. As you make changes to the notebook, the preview site will automatically reload."
  },
  {
    "objectID": "pages/SETUP.html#publish",
    "href": "pages/SETUP.html#publish",
    "title": "Setup",
    "section": "Publish",
    "text": "Publish\nIf you’ve improved the publication, consider contributing so we can update the hosted publication with your edits (big or small!). To get started, see our contributing guide."
  },
  {
    "objectID": "pages/CONTRIBUTING.html",
    "href": "pages/CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome improvements to this publication! If you’d like to improve or extend the publication, please submit a pull request. We’ll collaborate with you to incorporate your revisions. Alternatively, you’re welcome to leave a comment on the pub using Giscus.\n\nDid you spot any mistakes?\nDo you think an analysis is missing?\nDo you think the wording could be improved?\nDid you spot a typo or grammatical mistake?\n\nThese are just a few examples of revisions that we’d be happy to receive from you.\n\n\n\n\n\n\nNote\n\n\n\nTo learn about how we credit external collaborators, click here.\n\n\n\n\nIf you haven’t already, follow our setup guide to create a local copy of the code and compute environment.\n\n\n\nEdit index.ipynb to your liking.\n\n\n\nTo publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/CONTRIBUTING.html#getting-started",
    "href": "pages/CONTRIBUTING.html#getting-started",
    "title": "Contributing",
    "section": "",
    "text": "If you haven’t already, follow our setup guide to create a local copy of the code and compute environment."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#make-your-changes",
    "href": "pages/CONTRIBUTING.html#make-your-changes",
    "title": "Contributing",
    "section": "",
    "text": "Edit index.ipynb to your liking."
  },
  {
    "objectID": "pages/CONTRIBUTING.html#steps-before-publishing",
    "href": "pages/CONTRIBUTING.html#steps-before-publishing",
    "title": "Contributing",
    "section": "",
    "text": "To publish your revisions, we need you to open a pull request. And in order for us to merge your pull request, here’s what we’ll need from you in addition to your content changes.\nBegin with a clean branch (no uncommitted changes). Then run the notebook from the command line:\nmake execute\nThis command will update index.ipynb with the latest execution results.\nThen run make preview to see how the publication is rendering. Verify that your changes appear how you intend them to appear. If not, make the necessary changes and re-run make execute.\nOnce everything looks good, commit index.ipynb and all files in the _freeze/ directory.\nFinally, submit a pull request and we’ll work with you to merge your changes.\nOnce we approve and merge your pull request, we’ll publish a new version of the pub. We’ll notify you when this new version goes live at the hosted URL. Thanks for contributing!"
  },
  {
    "objectID": "pages/FAQ.html",
    "href": "pages/FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications.\n\n\n\nThere is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget.\n\n\n\nAll the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository.\n\n\n\nReproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started.\n\n\n\nWe welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "pages/FAQ.html#what-is-this",
    "href": "pages/FAQ.html#what-is-this",
    "title": "FAQ",
    "section": "",
    "text": "This notebook publication uses a format we’re experimenting with that treats a scientist’s working computational analysis as the publication itself, dissolving the separation that exists between code and publication. Our hypothesis is that this lets us publish faster, promote early-stage work, and increase reproducibility. For details, see our commentary on notebook publications."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-comment",
    "href": "pages/FAQ.html#how-can-i-comment",
    "title": "FAQ",
    "section": "",
    "text": "There is a comment section at the bottom of the pub, where you can read and contribute to any community discussion. Note that commenting requires a GitHub account and authorizing Giscus, a GitHub Discussions widget."
  },
  {
    "objectID": "pages/FAQ.html#where-is-the-datacode",
    "href": "pages/FAQ.html#where-is-the-datacode",
    "title": "FAQ",
    "section": "",
    "text": "All the code for this publication and its analysis are hosted on GitHub at this URL. Any associated data is either hosted or linked to from this repository."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-reproduce-this",
    "href": "pages/FAQ.html#how-can-i-reproduce-this",
    "title": "FAQ",
    "section": "",
    "text": "Reproducing this publication is as easy as issuing a few commands from the command line. Follow this setup guide to get started."
  },
  {
    "objectID": "pages/FAQ.html#how-can-i-contribute",
    "href": "pages/FAQ.html#how-can-i-contribute",
    "title": "FAQ",
    "section": "",
    "text": "We welcome improvements to this publication! Please see our guide for contributing."
  },
  {
    "objectID": "examples/demo.html",
    "href": "examples/demo.html",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#introduction",
    "href": "examples/demo.html#introduction",
    "title": "A brief syntax demo",
    "section": "",
    "text": "This notebook demos some key features. For a more extensive resource, see Quarto’s excellent documentation."
  },
  {
    "objectID": "examples/demo.html#text",
    "href": "examples/demo.html#text",
    "title": "A brief syntax demo",
    "section": "Text",
    "text": "Text\n\nHeaders\nh1 headers (# &lt;HEADER-TEXT&gt;) are reserved for the title of the pub, so use h2 (## &lt;HEADER-TEXT&gt;) for section titles and h3, h4, etc. for sub-sections.\n\n\nCallouts\nTo draw more attention to a piece of text, use callouts:\n\n\n\n\n\n\nImportant\n\n\n\nThe most effective way to see the rendered pub is to setup a live preview that re-renders the pub whenever you save this file. Do that with make preview.\n\n\n\n\nCitations & Footnotes\nTo cite something, add its bibtex entry to ref.bib and then cite it (Avasthi et al., 2024). Here’s another (Lin et al., 2023). For in-depth description of available citation syntax, visit Quarto’s documentation.\nAlso, don’t forget about footnotes1.\n1 To add additional information, like what you’re reading right now, use footnotes.\nTo create a multi-paragraph footnote, indent subsequent paragraphs. Footnotes can also cite things (Avasthi et al., 2024)."
  },
  {
    "objectID": "examples/demo.html#math",
    "href": "examples/demo.html#math",
    "title": "A brief syntax demo",
    "section": "Math",
    "text": "Math\nRender math2 using standard \\(\\LaTeX\\) syntax. Inline with $...$ and display with $$...$$.\n2 Quarto uses MathJax for math rendering.\\[\ne^{ \\pm i\\theta } = \\cos \\theta \\pm i\\sin \\theta\n\\tag{1}\\]\nEuler’s equation (Equation 1) is pretty."
  },
  {
    "objectID": "examples/demo.html#code",
    "href": "examples/demo.html#code",
    "title": "A brief syntax demo",
    "section": "Code",
    "text": "Code\nWrite code as you would in any Jupyter notebook.\n\ndef alertness(hours_sleep, coffees):\n    base_alertness = min(hours_sleep / 8 * 100, 100)\n    coffee_boost = min(coffees * 30, 60)\n    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\nprint(\"Alertness stats:\")\nprint(f\"4hrs sleep + 1 coffee: {alertness(4, 1)}%\")\nprint(f\"8hrs sleep + 0 coffee: {alertness(8, 0)}%\")\nprint(f\"2hrs sleep + 3 coffee: {alertness(2, 3)}%\")\n\nAlertness stats:\n4hrs sleep + 1 coffee: 70.0%\n8hrs sleep + 0 coffee: 100.0%\n2hrs sleep + 3 coffee: 85.0%\n\n\n\nVisibility & Placement\nSpecify per-block instructions with comments at the top of the code block.\nFold the code block (#| code-fold: true):\n\n\nSource code for this table\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [True, True, False], \"c\": [\"marco\", \"polo\", \"marco\"]})\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\nSuppress code block visibility while retaining the cell output (#| echo: false):\n\n\n\n\n\n\nNote\n\n\n\nThe code block below runs and the output is visible, but the code itself is absent from the rendering.\n\n\n\n\nThe code that generated this print statement is hidden.\n\n\nRender the output in different places, like in the right margin (#| column: margin):\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\nIn general, content placement in highly customizable. For more options, see this Quarto resource.\n\n\nAnnotation\nYou can annotate lines of code. Lines will reveal their annotation when the user hovers over the circled number on the right hand side of the code block.\n\ndef alertness(hours_sleep, coffees):\n1    base_alertness = min(hours_sleep / 8 * 100, 100)\n2    coffee_boost = min(coffees * 20, 60)\n3    total = min(base_alertness + coffee_boost, 100)\n    return round(total, 1)\n\n\n1\n\nScale to percentage, cap at 100\n\n2\n\nEach coffee adds 20%, max 60% boost\n\n3\n\nCap total at 100%\n\n\n\n\nFor details, see Quarto’s code annotation documentation.\n\n\nCodebase\nYou can choose to either fold (#| code-fold: true) or supress (#| echo: false) code snippets that distract from the narrative. However, if you’ve written an extensive amount of code, it may be more practical to define it in a package that this notebook imports from, rather than defining it in the notebook itself. This project is already set up to import from packages found in the src/ directory, so place any such code there. As an example, this code block imports code from a placeholder analysis package found at src/analysis.\n\n1from analysis import polo_if_marco\n\npolo_if_marco(\"marco\")\n\n\n1\n\nSource code\n\n\n\n\n'polo'\n\n\nIf you want to package any of your code for the purposes of simplifying this notebook, replace the contents of src/analysis/ with your own package."
  },
  {
    "objectID": "examples/demo.html#figures-tables",
    "href": "examples/demo.html#figures-tables",
    "title": "A brief syntax demo",
    "section": "Figures & Tables",
    "text": "Figures & Tables\n\nCaptions & Labeling\nIn general, if a cell output is a figure or table, you should caption and label it.\n\ndf\n\n\n\nTable 1: This is a small table.\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      True\n      marco\n    \n    \n      1\n      2\n      True\n      polo\n    \n    \n      2\n      3\n      False\n      marco\n    \n  \n\n\n\n\n\n\n\nThis is how you reference Table 1.\n\n\n\n\n\n\nNote\n\n\n\nIf the cell output is a table, the label ID should be prefixed with tbl-. If it’s a figure, prefix with fig-.\nFor example, a table could be captioned and labeled with:\n#| label: tbl-small-table\n#| tbl-cap: \"This is a small table.\"\nAnd a figure could be captioned and labeled with:\n#| label: fig-some-figure\n#| fig-cap: \"This is some figure.\"\n\n\nIf your code block produces several plots, you can subcaption each:\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef random_plot():\n    plt.figure()\n    plt.scatter(np.random.rand(10), np.random.rand(10), marker=\"o\")\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\nfor _ in range(4):\n    random_plot()\n\n\n\n\n\n\n\n\n\n\n\n(a) This is the first plot.\n\n\n\n\n\n\n\n\n\n\n\n(b) This is the second.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The third.\n\n\n\n\n\n\n\n\n\n\n\n(d) And finally, here’s the fourth.\n\n\n\n\n\n\n\nFigure 1: A panel of scatter plots.\n\n\n\nFigure 1 is just one example layout for multi-panel figures. For more customization options, see Quarto’s documentation on figures.\n\n\nInteractivity\nInteractive widgets can be used. For example, Plotly:\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\npx.scatter(\n    df,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    animation_frame=\"year\",\n    animation_group=\"country\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=55,\n    range_x=[100, 100000],\n    range_y=[25, 90],\n)\n\n                                                \n\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s possible that your local preview fails to render the above widget, and you instead see something to the effect of:\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\nIf you want to see how your widget renders within the pub, run make execute and then start a new preview (make preview).\n\n\n\n\nStatic images\nYou can include images that are not rendered by code, either as formal figures or standalone images.\n\n\n\n\n\n\nFigure 2: Example of a static image being used as a figure.\n\n\n\n\n\n\n\nExample of a standalone static image."
  }
]